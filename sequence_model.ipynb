{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Semantic Text Similarity\n",
    "Este modelo utiliza gensim para convertir pares de vectores + puntuaciones en vectores (word embeddings).\n",
    "Dado un dataset, infiere la puntuación de similitud entre ambas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:39:15.714539Z",
     "start_time": "2023-05-24T12:39:14.510628Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Requisitos\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:39:15.716990Z",
     "start_time": "2023-05-24T12:39:15.715653Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tipado\n",
    "from typing import Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T07:41:52.327621Z",
     "start_time": "2023-05-24T07:41:52.320007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modelos pre-entrenados\n",
    "WV_MODEL_PATH = \"/Users/salva/Downloads/cc.ca.300.bin.gz\"\n",
    "# from gensim.models import Word2Vec\n",
    "# wv_model = Word2Vec.load('path_to_word2vec_model').wv\n",
    "# from gensim.models import fasttext\n",
    "# wv_model = fasttext.load_facebook_vectors(WV_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:37.942653Z",
     "start_time": "2023-05-24T12:39:17.092994Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cc.ca.gensim.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Llavors podeu carregar el model com a mmap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfasttext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastTextKeyedVectors\n\u001b[1;32m----> 3\u001b[0m wv_model \u001b[38;5;241m=\u001b[39m \u001b[43mFastTextKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc.ca.gensim.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\gensim\\models\\fasttext.py:1001\u001b[0m, in \u001b[0;36mFastTextKeyedVectors.load\u001b[1;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fname_or_handle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    983\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a previously saved `FastTextKeyedVectors` model.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    999\u001b[0m \n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastTextKeyedVectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\gensim\\utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\gensim\\utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\smart_open\\smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.ca.gensim.bin'"
     ]
    }
   ],
   "source": [
    "# Llavors podeu carregar el model com a mmap\n",
    "from gensim.models.fasttext import FastTextKeyedVectors\n",
    "wv_model = FastTextKeyedVectors.load('cc.ca.gensim.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T07:53:55.145748Z",
     "start_time": "2023-05-24T07:53:55.137897Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejemplo de 10 pares de oraciones con puntuación de similitud asociada\n",
    "input_pairs = [\n",
    "    ('M\\'agrada el futbol', 'Disfruto veient partits de futbol', 4),\n",
    "    ('El cel està despejat', 'Fa un dia bonic', 4.5),\n",
    "    ('M\\'encanta viatjar', 'Explorar nous llocs és una passió', 3.5),\n",
    "    ('Prefereixo l\\'estiu', 'No m\\'agrada el fred de l\\'hivern', 2.5),\n",
    "    ('Tinc gana', 'Què hi ha per sopar?', 2),\n",
    "    ('La música em relaxa', 'Escoltar música és una teràpia', 3),\n",
    "    ('El llibre és emocionant', 'No puc deixar de llegir-lo', 4),\n",
    "    ('M\\'agrada la pizza', 'És el meu menjar preferit', 4.5),\n",
    "    ('Estic cansat', 'Necessito fer una migdiada', 1.5),\n",
    "    ('Avui fa molta calor', 'És un dia sofocant', 3.5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:37.944379Z",
     "start_time": "2023-05-24T12:40:37.941065Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REMAP_EMBEDDINGS: bool = True\n",
    "USE_PRETRAINED: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.564558Z",
     "start_time": "2023-05-24T12:40:37.948890Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Datos reales\n",
    "TRAIN_DATA_FILE: str = \"/Users/salva/Downloads/train.tsv\"\n",
    "import pandas as pd\n",
    "tsv_data = pd.read_csv(TRAIN_DATA_FILE, sep='\\t', header=None, usecols=[1, 2, 3])\n",
    "input_pairs = tsv_data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.643938Z",
     "start_time": "2023-05-24T12:40:38.570613Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(sentence_1) for sentence_1, _, _ in input_pairs]\n",
    "sentences_2_preproc = [simple_preprocess(sentence_2) for _, sentence_2, _ in input_pairs]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "diccionario = Dictionary(sentences_pairs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.648117Z",
     "start_time": "2023-05-24T12:40:38.645076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Max Len:\", max([len(s) for s in sentences_1_preproc]), max([len(s) for s in sentences_2_preproc]))\n",
    "print(list(diccionario.doc2idx(sentences_1_preproc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:54:04.894847Z",
     "start_time": "2023-05-24T12:54:04.890375Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_word_embeddings(\n",
    "        sentence: str,\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map to word-embedding indices\n",
    "    :param sentence:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence_preproc = simple_preprocess(sentence)\n",
    "    _vectors = np.zeros(sequence_len, dtype=np.int32)\n",
    "    index = 0\n",
    "    for word in sentence_preproc:\n",
    "        if fixed_dictionary is not None:\n",
    "            if word in fixed_dictionary.token2id:\n",
    "                # Sumo 1 porque el valor 0 está reservado a padding\n",
    "                _vectors[index] = fixed_dictionary.token2id[word] + 1\n",
    "                index += 1\n",
    "        else:\n",
    "            if word in wv_model.key_to_index:\n",
    "                _vectors[index] = wv_model.key_to_index[word] + 1\n",
    "                index += 1\n",
    "    return _vectors\n",
    "\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
    "    :param sentence_pairs:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        vector1 = map_word_embeddings(sentence_1, sequence_len, fixed_dictionary)\n",
    "        vector2 = map_word_embeddings(sentence_2, sequence_len, fixed_dictionary)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T12:54:31.458466Z",
     "start_time": "2023-05-24T12:54:31.456474Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "mapped = map_pairs(input_pairs, fixed_dictionary=diccionario if REMAP_EMBEDDINGS else None)\n",
    "# for vectors, similitud in mapped:\n",
    "#     print(f\"Pares de vectores: {vectors[0].shape}, {vectors[1].shape}\")\n",
    "#     print(f\"Puntuación de similitud: {similitud}\")\n",
    "print(mapped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:08.667870Z",
     "start_time": "2023-05-24T15:30:08.659838Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "def build_and_compile_model(\n",
    "        input_length: int = 32,\n",
    "        hidden_size: int = 64,\n",
    "        dictionary_size: int = 1000,\n",
    "        embedding_size: int = 16,\n",
    "        pretrained_weights: Optional[np.ndarray] = None,\n",
    "        learning_rate: float = 0.001,\n",
    "        trainable: bool = False,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Este es un modelo muy básico. Hace lo mismo que el modelo single_vector. La puntuación es mejor por no eliminar stopwords.\n",
    "    :param input_length:\n",
    "    :param hidden_size:\n",
    "    :param dictionary_size:\n",
    "    :param embedding_size:\n",
    "    :param pretrained_weights:\n",
    "    :param learning_rate:\n",
    "    :param trainable:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    else:\n",
    "        dictionary_size = pretrained_weights.shape[0]\n",
    "        embedding_size = pretrained_weights.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True,\n",
    "            embeddings_initializer=initializer, trainable=trainable, )\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "    _pooled_1, _pooled_2 = pooling(_embedded_1, mask=_input_mask_1), pooling(_embedded_2, mask=_input_mask_2)\n",
    "\n",
    "    # Compute the cosine distance\n",
    "    projected_1 = tf.linalg.l2_normalize(_pooled_1, axis=1, )\n",
    "    projected_2 = tf.linalg.l2_normalize(_pooled_2, axis=1, )\n",
    "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=output,)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:10.672062Z",
     "start_time": "2023-05-24T15:30:10.667865Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "import tensorflow as tf\n",
    "def build_and_compile_model_2(\n",
    "        input_length: int = 32,\n",
    "        dictionary_size: int = 1000,\n",
    "        embedding_size: int = 16,\n",
    "        learning_rate: float = 0.001,\n",
    "        trainable: bool = False,\n",
    "        pretrained_weights:bool = None\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Este es un modelo algo más avanzado. Calcula internamente una media ponderada de los word embeddings. Calcula también la proyección.\n",
    "    :param input_length:\n",
    "    :param hidden_size:\n",
    "    :param dictionary_size:\n",
    "    :param embedding_size:\n",
    "    :param pretrained_weights:\n",
    "    :param learning_rate:\n",
    "    :param trainable:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    else:\n",
    "        dictionary_size = pretrained_weights.shape[0]\n",
    "        embedding_size = pretrained_weights.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True,\n",
    "            embeddings_initializer=initializer, trainable=trainable, )\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "\n",
    "    # Compute custom weights\n",
    "    weights_computation = tf.keras.layers.Dense(1, name=\"weight_computation\")\n",
    "    dropout = tf.keras.layers.Dropout(0.2, name=\"dropout_in\")\n",
    "    _weights_1 = weights_computation(dropout(_embedded_1))\n",
    "    weights_1 = tf.squeeze(_weights_1, axis=[-1])\n",
    "    _weights_2 = weights_computation(dropout(_embedded_2))\n",
    "    weights_2 = tf.squeeze(_weights_2, axis=[-1])\n",
    "    # Define softmax\n",
    "    softmax = tf.keras.layers.Softmax(name=\"weighted_sum_softmax\")\n",
    "    scores_1 = softmax(weights_1, mask=_input_mask_1)\n",
    "    _pooled_1 = tf.math.reduce_sum(_embedded_1 * tf.expand_dims(scores_1, axis=-1), axis=1)\n",
    "    scores_2 = softmax(weights_2, mask=_input_mask_2)\n",
    "    _pooled_2 = tf.math.reduce_sum(_embedded_2 * tf.expand_dims(scores_2, axis=-1) , axis=1)\n",
    "    # Compute the distance\n",
    "    dense_output = tf.keras.layers.Dense(1)\n",
    "    dropout_out = tf.keras.layers.Dropout(0.2, name=\"dropout_out\")\n",
    "    projected_1 = tf.linalg.l2_normalize(_pooled_1, axis=1, )\n",
    "    projected_2 = tf.linalg.l2_normalize(_pooled_2, axis=1, )\n",
    "    output = dense_output(dropout_out(projected_1 * projected_2), )\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=output,)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:10.854703Z",
     "start_time": "2023-05-24T15:30:10.850423Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir constantes de entrenamiento\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64\n",
    "train_val_split: float = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.041186Z",
     "start_time": "2023-05-24T15:30:11.033967Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtener x_train e y_train\n",
    "train_slice: int = int(len(mapped) * train_val_split)\n",
    "\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
    "    :param pair_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.row_stack(_x_1), np.row_stack(_x_2)), np.array(_y)\n",
    "\n",
    "# Obtener las listas de train y test\n",
    "x_train, y_train = pair_list_to_x_y(mapped[:train_slice])\n",
    "x_val, y_val = pair_list_to_x_y(mapped[train_slice:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.209237Z",
     "start_time": "2023-05-24T15:30:11.198383Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparar los conjuntos de datos de entrenamiento y validación\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.518333Z",
     "start_time": "2023-05-24T15:30:11.415410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained_weights: Optional[np.ndarray] = None\n",
    "if USE_PRETRAINED:\n",
    "    if REMAP_EMBEDDINGS:\n",
    "        pretrained_weights = np.zeros(\n",
    "            (len(diccionario.token2id) + 1, wv_model.vector_size),  dtype=np.float32)\n",
    "        for token, _id in diccionario.token2id.items():\n",
    "            if token in wv_model:\n",
    "                pretrained_weights[_id + 1] = wv_model[token]\n",
    "            else:\n",
    "                # In W2V, OOV will not have a representation. We will use 0.\n",
    "                pass\n",
    "    else:\n",
    "        # Not recommended (this will consume A LOT of RAM)\n",
    "        pretrained_weights = np.zeros((wv_model.vectors.shape[0] + 1, wv_model.vector_size,),  dtype=np.float32)\n",
    "        pretrained_weights[1:, :] = wv_model.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.597751Z",
     "start_time": "2023-05-24T15:30:11.594305Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:26.688759Z",
     "start_time": "2023-05-24T15:30:11.768236Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model = build_and_compile_model(pretrained_weights=pretrained_weights, )\n",
    "# Entrenar el modelo\n",
    "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:27.726622Z",
     "start_time": "2023-05-24T15:30:27.655446Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred = model.predict(x_val)\n",
    "# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlación de Pearson\n",
    "print(f\"Correlación de Pearson: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:34.411112Z",
     "start_time": "2023-05-24T15:30:34.409004Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip(y_pred, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T15:20:48.758153Z",
     "start_time": "2023-05-24T15:20:48.718750Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
