{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C6R8Swq8m7f"
      },
      "source": [
        "# Pràctica 4\n",
        "### Part I : Entrenament de models Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:00:29.701949Z",
          "start_time": "2023-05-31T17:00:23.336155Z"
        },
        "id": "1kdMxSds8m7h",
        "outputId": "7606a961-7a32-46bf-d6ef-d38f7f220e4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
            "[notice] To update, run: C:\\Users\\adria\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ca-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.5.0/ca_core_news_sm-3.5.0-py3-none-any.whl (19.6 MB)\n",
            "     --------------------------------------- 19.6/19.6 MB 38.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ca-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.31.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.10.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.23.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (65.5.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.6.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.1.1)\n",
            "Installing collected packages: ca-core-news-sm\n",
            "Successfully installed ca-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ca_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download ca_core_news_sm\n",
        "import spacy\n",
        "nlp = spacy.load('ca_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:08:44.005084Z",
          "start_time": "2023-05-31T17:00:30.215642Z"
        },
        "colab": {
          "referenced_widgets": [
            "3402ce7ea5f0473f8108e63ba971528e",
            "6318e79f477945a58838d50befe19df4",
            "915c1f7c98f74035983a90dc0e2c0ab7",
            "facac4f52c1045dda49157cbf06c76cf",
            "6031330d7a6c40aea7adbea8c2c2801c"
          ]
        },
        "id": "Q1Hp82XD8m7h",
        "outputId": "298e65da-3ee9-45a8-e8f0-2b53da3612f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3402ce7ea5f0473f8108e63ba971528e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6318e79f477945a58838d50befe19df4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.93k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset catalan_general_crawling/default to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "915c1f7c98f74035983a90dc0e2c0ab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "facac4f52c1045dda49157cbf06c76cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset catalan_general_crawling downloaded and prepared to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6031330d7a6c40aea7adbea8c2c2801c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ZN0P-78m7i"
      },
      "outputs": [],
      "source": [
        "with open('data/ca_gen_crwlng.txt','w',encoding='UTF-8') as f:\n",
        "    for i in dataset['train']['text']:\n",
        "        f.write(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JibG27ST8m7i"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from gensim.utils import simple_preprocess\n",
        "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
        "\n",
        "\n",
        "def preprocess(sentence: str) -> List[str]:\n",
        "    preprocessed = simple_preprocess(sentence)\n",
        "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAXGuZpa8m7i"
      },
      "outputs": [],
      "source": [
        "def obtain_words(filepath:str) -> list[list[str]]:\n",
        "    if filepath[-4:] != '.txt':\n",
        "        raise Exception('Incorrect file path/name')\n",
        "\n",
        "    txt_crps = []\n",
        "    with open(filepath,'r',encoding='UTF-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for l in lines:\n",
        "            txt_crps.append(preprocess(l))\n",
        "\n",
        "    return txt_crps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oqMe-N-8m7i"
      },
      "outputs": [],
      "source": [
        "txt_crps100 = obtain_words('data/ca_gen_crwlng100M.txt')\n",
        "txt_crps500 = obtain_words('data/ca_gen_crwlng500M.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8W_BLi98m7j"
      },
      "source": [
        "##### Create Word2Vec models with the 2 sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfD2XYGF8m7j"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "model100 = word2vec.Word2Vec(txt_crps100 , vector_size=100, window=5, min_count=10, workers=4, epochs=25, sg=1)\n",
        "model500 = word2vec.Word2Vec(txt_crps500 , vector_size=100, window=5, min_count=10, workers=4, epochs=25, sg=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ1y6zRK8m7j"
      },
      "source": [
        "##### Save the models to easy load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAGj-0Hq8m7j"
      },
      "outputs": [],
      "source": [
        "model100.wv.save_word2vec_format('data/word2vec100.bin', binary=True)\n",
        "model500.wv.save_word2vec_format('data/word2vec500.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_wGX5HP8m7j"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "wv100 = KeyedVectors.load_word2vec_format('data/word2vec100.bin', binary=True)\n",
        "wv500 = KeyedVectors.load_word2vec_format('data/word2vec500.bin', binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OXa5_sMn8m7k"
      },
      "source": [
        "## Text similarity training and comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhDnDtW18m7k"
      },
      "source": [
        "### Obtain the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d16735db18c34f5ba67e95abe726bf1c"
          ]
        },
        "id": "BgyI7KRR8m7k",
        "outputId": "46280675-b159-42c5-c063-3ec93caff855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset sts-ca (C:/Users/pelot/.cache/huggingface/datasets/projecte-aina___sts-ca/StsCa/1.0.2/bad37fb7fb0f06f3d2316e29637293b25160a93a24f36f1974f21313ac2f3342)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d16735db18c34f5ba67e95abe726bf1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"projecte-aina/sts-ca\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PBlhu8Eg8m7k"
      },
      "source": [
        "### Obtain the Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDhOJAu8m7k",
        "outputId": "9f516150-b25c-40db-a13e-5e08d4bfef77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 2073\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sOeh8Av8m7k"
      },
      "outputs": [],
      "source": [
        "## Imports necessaris\n",
        "##!python3 -m spacy download ca_core_news_md\n",
        "##!python3 -m spacy download ca_core_news_trf\n",
        "\n",
        "##!python3 -m pip install spacy-transformers\n",
        "##!python3 -m pip install sentence_transformers\n",
        "\n",
        "# Requisitos\n",
        "\n",
        "from gensim.models import TfidfModel, KeyedVectors, fasttext\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np\n",
        "# Tipado\n",
        "from typing import Tuple, List\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from scipy.stats import pearsonr\n",
        "from gensim.models.fasttext import FastTextKeyedVectors\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPqAOO7h8m7l",
        "outputId": "a7b96f2c-6c12-409f-908d-e75c5c31df32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfidf creadoo\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cargar stopwords en Catalan\n",
        "# STOPWORDS_CA = {\"a\", \"abans\", \"ací\", \"ah\", \"així\", \"això\", \"al\", \"aleshores\", \"algun\", \"alguna\", \"algunes\", \"alguns\", \"alhora\", \"allà\", \"allí\", \"allò\", \"als\", \"altra\", \"altre\", \"altres\", \"amb\", \"ambdues\", \"ambdós\", \"anar\", \"ans\", \"apa\", \"aquell\", \"aquella\", \"aquelles\", \"aquells\", \"aquest\", \"aquesta\", \"aquestes\", \"aquests\", \"aquí\", \"baix\", \"bastant\", \"bé\", \"cada\", \"cadascuna\", \"cadascunes\", \"cadascuns\", \"cadascú\", \"com\", \"consegueixo\", \"conseguim\", \"conseguir\", \"consigueix\", \"consigueixen\", \"consigueixes\", \"contra\", \"d'un\", \"d'una\", \"d'unes\", \"d'uns\", \"dalt\", \"de\", \"del\", \"dels\", \"des\", \"des de\", \"després\", \"dins\", \"dintre\", \"donat\", \"doncs\", \"durant\", \"e\", \"eh\", \"el\", \"elles\", \"ells\", \"els\", \"em\", \"en\", \"encara\", \"ens\", \"entre\", \"era\", \"erem\", \"eren\", \"eres\", \"es\", \"esta\", \"estan\", \"estat\", \"estava\", \"estaven\", \"estem\", \"esteu\", \"estic\", \"està\", \"estàvem\", \"estàveu\", \"et\", \"etc\", \"ets\", \"fa\", \"faig\", \"fan\", \"fas\", \"fem\", \"fer\", \"feu\", \"fi\", \"fins\", \"fora\", \"gairebé\", \"ha\", \"han\", \"has\", \"haver\", \"havia\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \"i\", \"igual\", \"iguals\", \"inclòs\", \"ja\", \"jo\", \"l'hi\", \"la\", \"les\", \"li\", \"li'n\", \"llarg\", \"llavors\", \"m'he\", \"ma\", \"mal\", \"malgrat\", \"mateix\", \"mateixa\", \"mateixes\", \"mateixos\", \"me\", \"mentre\", \"meu\", \"meus\", \"meva\", \"meves\", \"mode\", \"molt\", \"molta\", \"moltes\", \"molts\", \"mon\", \"mons\", \"més\", \"n'he\", \"n'hi\", \"ne\", \"ni\", \"no\", \"nogensmenys\", \"només\", \"nosaltres\", \"nostra\", \"nostre\", \"nostres\", \"o\", \"oh\", \"oi\", \"on\", \"pas\", \"pel\", \"pels\", \"per\", \"per que\", \"perquè\", \"però\", \"poc\", \"poca\", \"pocs\", \"podem\", \"poden\", \"poder\", \"podeu\", \"poques\", \"potser\", \"primer\", \"propi\", \"puc\", \"qual\", \"quals\", \"quan\", \"quant\", \"que\", \"quelcom\", \"qui\", \"quin\", \"quina\", \"quines\", \"quins\", \"què\", \"s'ha\", \"s'han\", \"sa\", \"sabem\", \"saben\", \"saber\", \"sabeu\", \"sap\", \"saps\", \"semblant\", \"semblants\", \"sense\", \"ser\", \"ses\", \"seu\", \"seus\", \"seva\", \"seves\", \"si\", \"sobre\", \"sobretot\", \"soc\", \"solament\", \"sols\", \"som\", \"son\", \"sons\", \"sota\", \"sou\", \"sóc\", \"són\", \"t'ha\", \"t'han\", \"t'he\", \"ta\", \"tal\", \"també\", \"tampoc\", \"tan\", \"tant\", \"tanta\", \"tantes\", \"te\", \"tene\", \"tenim\", \"tenir\", \"teniu\", \"teu\", \"teus\", \"teva\", \"teves\", \"tinc\", \"ton\", \"tons\", \"tot\", \"tota\", \"totes\", \"tots\", \"un\", \"una\", \"unes\", \"uns\", \"us\", \"va\", \"vaig\", \"vam\", \"van\", \"vas\", \"veu\", \"vosaltres\", \"vostra\", \"vostre\", \"vostres\", \"érem\", \"éreu\", \"és\", \"éssent\", \"últim\", \"ús\"}\n",
        "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
        "\n",
        "# Definir función de pre-procesado\n",
        "def preprocess(sentence: str) -> List[str]:\n",
        "    preprocessed = simple_preprocess(sentence)\n",
        "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
        "    return preprocessed\n",
        "\n",
        "## Introducir los datos de train y de validación\n",
        "input_pairs = list(zip(dataset[\"train\"][\"sentence1\"], dataset[\"train\"][\"sentence2\"], dataset[\"train\"][\"label\"]))\n",
        "input_pairs_val = list(zip(dataset[\"validation\"][\"sentence1\"], dataset[\"validation\"][\"sentence2\"], dataset[\"validation\"][\"label\"]))\n",
        "\n",
        "\n",
        "# Preprocesamiento de las oraciones y creación del diccionario\n",
        "sentences_1_preproc = [preprocess(sentence_1) for sentence_1, _, _ in input_pairs]\n",
        "sentences_2_preproc = [preprocess(sentence_2) for _, sentence_2, _ in input_pairs]\n",
        "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
        "# Versión aplanada para poder entrenar el modelo\n",
        "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
        "\n",
        "diccionario = Dictionary(sentences_pairs_flattened)\n",
        "\n",
        "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
        "modelo_tfidf = TfidfModel(corpus)\n",
        "\n",
        "def creacio_one_hot(sentence, diccionario:Dictionary):\n",
        "    vector1 = np.zeros(len(diccionario.token2id), dtype=np.float)\n",
        "    bow1 = diccionario.doc2bow(sentence)\n",
        "    for index, count in bow1:\n",
        "        vector1[index] = count\n",
        "\n",
        "    return vector1\n",
        "\n",
        "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
        "    bow = dictionary.doc2bow(sentence_preproc)\n",
        "    tf_idf = tf_idf_model[bow]\n",
        "    vectors, weights = [], []\n",
        "    for word_index, weight in tf_idf:\n",
        "        word = dictionary.get(word_index)\n",
        "        if word in wv_model:\n",
        "            vectors.append(wv_model[word])\n",
        "            weights.append(weight)\n",
        "    return vectors, weights\n",
        "\n",
        "def map_pairs(\n",
        "        sentence_pairs: List[Tuple[str, str, float]],\n",
        "        dictionary: Dictionary = None,\n",
        "        tf_idf_model: TfidfModel = None,\n",
        "        one_hot: bool = None,\n",
        "        Spasii : bool = None,\n",
        "        RobertA: bool = None,\n",
        "        RobertA_Mean: bool = None,\n",
        "        wv100: bool = None,\n",
        "        wv500: bool = None,\n",
        "        w2v_pre_mean: bool = None,\n",
        "        RobertA_finetuned: bool = None\n",
        ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
        "    \"\"\"\n",
        "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
        "    :param sentence_pairs:\n",
        "    :param dictionary:\n",
        "    :param tf_idf_model:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if Spasii:\n",
        "        nlp = spacy.load(\"ca_core_news_md\")\n",
        "    if RobertA:\n",
        "        nlp = spacy.load(\"ca_core_news_trf\")\n",
        "\n",
        "    if wv100:\n",
        "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec100.bin\", binary = True)\n",
        "\n",
        "    if wv500:\n",
        "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec500.bin\", binary = True)\n",
        "\n",
        "    if w2v_pre_mean or tf_idf_model:\n",
        "        wv_model = FastTextKeyedVectors.load('data/model.bin', mmap='r')\n",
        "\n",
        "    if RobertA_finetuned:\n",
        "        model_name = 'stsb-roberta-base'\n",
        "        model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Mapeo de los pares de oraciones a pares de vectores\n",
        "    pares_vectores = []\n",
        "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
        "        sentence_1_preproc = preprocess(sentence_1)\n",
        "        sentence_2_preproc = preprocess(sentence_2)\n",
        "        # Si usamos TF-IDF\n",
        "        if tf_idf_model is not None:\n",
        "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
        "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, wv_model=wv_model )\n",
        "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model,wv_model=wv_model )\n",
        "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
        "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
        "\n",
        "        elif wv100 is not None:\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "\n",
        "        elif wv500 is not None:\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "\n",
        "        elif one_hot is not None:\n",
        "            vector1 = creacio_one_hot(sentence=sentence_1_preproc, diccionario=dictionary)\n",
        "            vector2 = creacio_one_hot(sentence=sentence_2_preproc, diccionario= dictionary)\n",
        "            \n",
        "        elif Spasii is not None:\n",
        "            vector1 = nlp(\" \".join(sentence_1_preproc)).vector\n",
        "            vector2 = nlp(\" \".join(sentence_2_preproc)).vector\n",
        "\n",
        "        elif RobertA is not None:\n",
        "            vector1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
        "            vector2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
        "\n",
        "        elif RobertA_Mean is not None:\n",
        "            vectors1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
        "            vectors2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
        "\n",
        "            vector1 = np.average(vectors1, axis=0)\n",
        "            vector2 = np.average(vectors2, axis=0)\n",
        "\n",
        "        elif RobertA_finetuned is not None:\n",
        "            vector1 = model.encode([\" \".join(sentence_1_preproc)])\n",
        "            vector2 = model.encode([\" \".join(sentence_2_preproc)])\n",
        "            \n",
        "        elif w2v_pre_mean is not None:\n",
        "            # Cálculo del promedio de los word embeddings\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "        # Añadir a la lista\n",
        "        pares_vectores.append(((vector1, vector2), similitud))\n",
        "    return pares_vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqBiQ_7R8m7l"
      },
      "source": [
        "Fem la funció de la creació del model y dels hiperparámetres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyNL-8mQ8m7m"
      },
      "outputs": [],
      "source": [
        "def build_and_compile_model(hidden_size: int = 128, embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
        "    # Capa de entrada para los pares de vectores\n",
        "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
        "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
        "\n",
        "    # Capa oculta\n",
        "    first_projection = tf.keras.layers.Dense(\n",
        "        embedding_size,\n",
        "        # activation='tanh',\n",
        "        kernel_initializer=tf.keras.initializers.Identity(),\n",
        "        bias_initializer=tf.keras.initializers.Zeros(),\n",
        "    )\n",
        "    projected_1 = first_projection(input_1)\n",
        "    projected_2 = first_projection(input_2)\n",
        "\n",
        "    # Compute the cosine distance\n",
        "    projected_1 = tf.linalg.l2_normalize(projected_1, axis=1, )\n",
        "    projected_2 = tf.linalg.l2_normalize(projected_2, axis=1, )\n",
        "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
        "\n",
        "    # Definir el modelo con las capas de entrada y salida\n",
        "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(loss='mean_absolute_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definir constantes de entrenamiento\n",
        "batch_size: int = 64\n",
        "num_epochs: int = 64\n",
        "\n",
        "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
        "    :param pair_list:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    _x, _y = zip(*pair_list)\n",
        "    _x_1, _x_2 = zip(*_x)\n",
        "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )\n",
        "\n",
        "\n",
        "def create_and_train_and_evaluate_model(mapped, mapped_val):\n",
        "    # Obtener las listas de train y test\n",
        "    x_train, y_train = pair_list_to_x_y(mapped)\n",
        "    x_val, y_val = pair_list_to_x_y(mapped_val)\n",
        "\n",
        "    # Preparar los conjuntos de datos de entrenamiento y validación\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Construir y compilar el modelo\n",
        "    model = build_and_compile_model()\n",
        "    # tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, )\n",
        "    print(model.summary())\n",
        "    # Entrenar el modelo\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "    y_pred: tf.RaggedTensor = model.predict(x_val)\n",
        "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
        "    correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
        "    # Imprimir el coeficiente de correlación de Pearson\n",
        "    print(f\"Correlación de Pearson: {correlation}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zvHCfM7_8m7m"
      },
      "source": [
        "### Compare results with different word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZtqXFR8m7m"
      },
      "source": [
        "#### 0. Model de Word2Vec entrenats per nosaltres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A3IweZK8m7m"
      },
      "source": [
        "##### 0.1. Model amb 100 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZENU8kuc8m7m"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, wv100=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, wv100=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n7zMRP38m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-pSYQe8m7n"
      },
      "source": [
        "##### 0.2. Model amb 500 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxx6u1Ig8m7n"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, wv500=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, wv500=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmOYMJ048m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QF0EG-AD8m7n"
      },
      "source": [
        "#### 1. One Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z55O8jG8m7n"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, one_hot=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, one_hot=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoNQhiIg8m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dmfqVPvY8m7n"
      },
      "source": [
        "#### 2. Models de Word2Vec/GloVe pre-entrenats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FGELWVb88m7n"
      },
      "source": [
        "##### 2.1.Word2Vec + Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvjn7pJL8m7o"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, w2v_pre_mean=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, w2v_pre_mean=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHRxNJ_J8m7o"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oDA0wr-X8m7o"
      },
      "source": [
        "##### 2.2.Word2Vec + Mean ponderada (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8zH2ip08m7o"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSMyClBN8m7o"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GsHl1GQD8m72"
      },
      "source": [
        "#### 3.Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd1Oecv8m72"
      },
      "source": [
        "Primer definim els embeddings amb el model de Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Q4cmr38m72"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, Spasii=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, Spasii=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juK2JoYz8m72"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SkeQ57oO8m72"
      },
      "source": [
        "#### 4.RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CBGLTSu28m73"
      },
      "source": [
        "##### 4.1.CLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjFoNCWB8m73"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClT4pyhg8m73"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d5qL6zGN8m73"
      },
      "source": [
        "##### 4.2.MEAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCxEI7aH8m73"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA_Mean=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA_Mean=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaDmE7D88m73"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9GYDsChM8m73"
      },
      "source": [
        "#### 5.RoBERTa FineTuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:48:42.264845Z",
          "start_time": "2023-05-31T17:11:49.470503Z"
        },
        "id": "OZd_7rU-8m74"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA_finetuned=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA_finetuned=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnrRULCk8m74"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uS_EyEA-8m74"
      },
      "source": [
        "## Train the same model with initiated trainable embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "841agYHG8m74"
      },
      "source": [
        "#### Random Embeddings (uniforme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8E5snLP8m74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kXfQwzz08m74"
      },
      "source": [
        "#### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiTeFTQe8m75"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9QkOR4Q28m75"
      },
      "source": [
        "## Analyze results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
