{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√†ctica 4\n",
    "### Part I : Entrenament de models Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6f9d9843fd34f0ab6d0dc0b56ad57f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/9.93k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b52bb8ce35c4a6d90267d215ba9cd07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset catalan_general_crawling/default to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e79c36f2f9c046a390fc5632028dbf0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fc35cd8e0fa467b91c763a8d5931987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset catalan_general_crawling downloaded and prepared to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "789e2a604a154a55b9abbe6c1b7711e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 1016113\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mgensim\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mmodels\u001B[39;00m \u001B[39mimport\u001B[39;00m word2vec\n\u001B[1;32m----> 2\u001B[0m model \u001B[39m=\u001B[39m word2vec\u001B[39m.\u001B[39;49mWord2Vec(dataset[\u001B[39m\"\u001B[39;49m\u001B[39mtrain\u001B[39;49m\u001B[39m\"\u001B[39;49m][\u001B[39m\"\u001B[39;49m\u001B[39mtext\u001B[39;49m\u001B[39m\"\u001B[39;49m] , vector_size\u001B[39m=\u001B[39;49m\u001B[39m100\u001B[39;49m, window\u001B[39m=\u001B[39;49m\u001B[39m5\u001B[39;49m, min_count\u001B[39m=\u001B[39;49m\u001B[39m10\u001B[39;49m, workers\u001B[39m=\u001B[39;49m\u001B[39m4\u001B[39;49m, epochs\u001B[39m=\u001B[39;49m\u001B[39m25\u001B[39;49m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\word2vec.py:429\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[39mif\u001B[39;00m corpus_iterable \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mor\u001B[39;00m corpus_file \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m    428\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[39m=\u001B[39mcorpus_iterable, corpus_file\u001B[39m=\u001B[39mcorpus_file, passes\u001B[39m=\u001B[39m(epochs \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m))\n\u001B[1;32m--> 429\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mbuild_vocab(corpus_iterable\u001B[39m=\u001B[39;49mcorpus_iterable, corpus_file\u001B[39m=\u001B[39;49mcorpus_file, trim_rule\u001B[39m=\u001B[39;49mtrim_rule)\n\u001B[0;32m    430\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtrain(\n\u001B[0;32m    431\u001B[0m         corpus_iterable\u001B[39m=\u001B[39mcorpus_iterable, corpus_file\u001B[39m=\u001B[39mcorpus_file, total_examples\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcorpus_count,\n\u001B[0;32m    432\u001B[0m         total_words\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcorpus_total_words, epochs\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mepochs, start_alpha\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39malpha,\n\u001B[0;32m    433\u001B[0m         end_alpha\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmin_alpha, compute_loss\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcompute_loss, callbacks\u001B[39m=\u001B[39mcallbacks)\n\u001B[0;32m    434\u001B[0m \u001B[39melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\word2vec.py:491\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[0;32m    453\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \n\u001B[0;32m    455\u001B[0m \u001B[39mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    488\u001B[0m \n\u001B[0;32m    489\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m    490\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[39m=\u001B[39mcorpus_iterable, corpus_file\u001B[39m=\u001B[39mcorpus_file, passes\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[1;32m--> 491\u001B[0m total_words, corpus_count \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mscan_vocab(\n\u001B[0;32m    492\u001B[0m     corpus_iterable\u001B[39m=\u001B[39;49mcorpus_iterable, corpus_file\u001B[39m=\u001B[39;49mcorpus_file, progress_per\u001B[39m=\u001B[39;49mprogress_per, trim_rule\u001B[39m=\u001B[39;49mtrim_rule)\n\u001B[0;32m    493\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcorpus_count \u001B[39m=\u001B[39m corpus_count\n\u001B[0;32m    494\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcorpus_total_words \u001B[39m=\u001B[39m total_words\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\word2vec.py:586\u001B[0m, in \u001B[0;36mWord2Vec.scan_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[39mif\u001B[39;00m corpus_file:\n\u001B[0;32m    584\u001B[0m     corpus_iterable \u001B[39m=\u001B[39m LineSentence(corpus_file)\n\u001B[1;32m--> 586\u001B[0m total_words, corpus_count \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001B[0;32m    588\u001B[0m logger\u001B[39m.\u001B[39minfo(\n\u001B[0;32m    589\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mcollected \u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m word types from a corpus of \u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m raw words and \u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m sentences\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[0;32m    590\u001B[0m     \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw_vocab), total_words, corpus_count\n\u001B[0;32m    591\u001B[0m )\n\u001B[0;32m    593\u001B[0m \u001B[39mreturn\u001B[39;00m total_words, corpus_count\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\word2vec.py:570\u001B[0m, in \u001B[0;36mWord2Vec._scan_vocab\u001B[1;34m(self, sentences, progress_per, trim_rule)\u001B[0m\n\u001B[0;32m    565\u001B[0m     logger\u001B[39m.\u001B[39minfo(\n\u001B[0;32m    566\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mPROGRESS: at sentence #\u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m, processed \u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m words, keeping \u001B[39m\u001B[39m%i\u001B[39;00m\u001B[39m word types\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[0;32m    567\u001B[0m         sentence_no, total_words, \u001B[39mlen\u001B[39m(vocab),\n\u001B[0;32m    568\u001B[0m     )\n\u001B[0;32m    569\u001B[0m \u001B[39mfor\u001B[39;00m word \u001B[39min\u001B[39;00m sentence:\n\u001B[1;32m--> 570\u001B[0m     vocab[word] \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n\u001B[0;32m    571\u001B[0m total_words \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39mlen\u001B[39m(sentence)\n\u001B[0;32m    573\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmax_vocab_size \u001B[39mand\u001B[39;00m \u001B[39mlen\u001B[39m(vocab) \u001B[39m>\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmax_vocab_size:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(dataset[\"train\"][\"text\"] , vector_size=100, window=5, min_count=10, workers=4, epochs=25, sg=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text similarity training and comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtain the Baseline model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare results with different word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. One Hot encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# limitar mida?, normalitzar?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Models de Word2Vec/GloVe pre-entrenats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.1.Word2Vec + Mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.Word2Vec + Mean ponderada (TF-IDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.RoBERTa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.1.CLS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (Amb spaCy, doc._.trf_data.tensors[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4.2.MEAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (Amb spaCy, doc._.trf_data.tensors[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.RoBERTa FineTuned"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the same model with initiated trainable embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Embeddings (uniforme)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
