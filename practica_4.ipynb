{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C6R8Swq8m7f"
   },
   "source": [
    "# Pràctica 4\n",
    "### Part I : Entrenament de models Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:00:29.701949Z",
     "start_time": "2023-05-31T17:00:23.336155Z"
    },
    "id": "1kdMxSds8m7h",
    "outputId": "7606a961-7a32-46bf-d6ef-d38f7f220e4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\adria\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.5.0/ca_core_news_sm-3.5.0-py3-none-any.whl (19.6 MB)\n",
      "     --------------------------------------- 19.6/19.6 MB 38.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ca-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.23.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: ca-core-news-sm\n",
      "Successfully installed ca-core-news-sm-3.5.0\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('ca_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download ca_core_news_sm\n",
    "import spacy\n",
    "nlp = spacy.load('ca_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:08:44.005084Z",
     "start_time": "2023-05-31T17:00:30.215642Z"
    },
    "colab": {
     "referenced_widgets": [
      "3402ce7ea5f0473f8108e63ba971528e",
      "6318e79f477945a58838d50befe19df4",
      "915c1f7c98f74035983a90dc0e2c0ab7",
      "facac4f52c1045dda49157cbf06c76cf",
      "6031330d7a6c40aea7adbea8c2c2801c"
     ]
    },
    "id": "Q1Hp82XD8m7h",
    "outputId": "298e65da-3ee9-45a8-e8f0-2b53da3612f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3402ce7ea5f0473f8108e63ba971528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6318e79f477945a58838d50befe19df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset catalan_general_crawling/default to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915c1f7c98f74035983a90dc0e2c0ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facac4f52c1045dda49157cbf06c76cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset catalan_general_crawling downloaded and prepared to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6031330d7a6c40aea7adbea8c2c2801c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1ZN0P-78m7i"
   },
   "outputs": [],
   "source": [
    "with open('data/ca_gen_crwlng.txt','w',encoding='UTF-8') as f:\n",
    "    for i in dataset['train']['text']:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JibG27ST8m7i"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from gensim.utils import simple_preprocess\n",
    "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
    "\n",
    "\n",
    "def preprocess(sentence: str) -> List[str]:\n",
    "    preprocessed = simple_preprocess(sentence)\n",
    "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAXGuZpa8m7i"
   },
   "outputs": [],
   "source": [
    "def obtain_words(filepath:str) -> list[list[str]]:\n",
    "    if filepath[-4:] != '.txt':\n",
    "        raise Exception('Incorrect file path/name')\n",
    "\n",
    "    txt_crps = []\n",
    "    with open(filepath,'r',encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            txt_crps.append(preprocess(l))\n",
    "\n",
    "    return txt_crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oqMe-N-8m7i"
   },
   "outputs": [],
   "source": [
    "txt_crps100 = obtain_words('data/ca_gen_crwlng100M.txt')\n",
    "txt_crps500 = obtain_words('data/ca_gen_crwlng500M.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8W_BLi98m7j"
   },
   "source": [
    "##### Create Word2Vec models with the 2 sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfD2XYGF8m7j"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model100 = word2vec.Word2Vec(txt_crps100 , vector_size=300, window=5, min_count=10, workers=4, epochs=25, sg=1)\n",
    "model500 = word2vec.Word2Vec(txt_crps500 , vector_size=300, window=5, min_count=10, workers=4, epochs=25, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ1y6zRK8m7j"
   },
   "source": [
    "##### Save the models to easy load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAGj-0Hq8m7j"
   },
   "outputs": [],
   "source": [
    "model100.wv.save_word2vec_format('data/word2vec100.bin', binary=True)\n",
    "model500.wv.save_word2vec_format('data/word2vec500.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w_wGX5HP8m7j"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv100 = KeyedVectors.load_word2vec_format('data/word2vec100.bin', binary=True)\n",
    "wv500 = KeyedVectors.load_word2vec_format('data/word2vec500.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OXa5_sMn8m7k"
   },
   "source": [
    "## Text similarity training and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhDnDtW18m7k"
   },
   "source": [
    "### Obtain the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d16735db18c34f5ba67e95abe726bf1c"
     ]
    },
    "id": "BgyI7KRR8m7k",
    "outputId": "46280675-b159-42c5-c063-3ec93caff855"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sts-ca (C:/Users/adria/.cache/huggingface/datasets/projecte-aina___sts-ca/StsCa/1.0.2/bad37fb7fb0f06f3d2316e29637293b25160a93a24f36f1974f21313ac2f3342)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50b0a9c7bac6405db325418bedf1355d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/sts-ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PBlhu8Eg8m7k"
   },
   "source": [
    "### Obtain the Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjDhOJAu8m7k",
    "outputId": "9f516150-b25c-40db-a13e-5e08d4bfef77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 2073\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "9sOeh8Av8m7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_md-3.5.0/ca_core_news_md-3.5.0-py3-none-any.whl (49.2 MB)\n",
      "     ---------------------------------------- 49.2/49.2 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from ca-core-news-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (65.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ca-core-news-md==3.5.0) (2.1.2)\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('ca_core_news_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-trf==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_trf-3.5.0/ca_core_news_trf-3.5.0-py3-none-any.whl (459.9 MB)\n",
      "     -------------------------------------- 459.9/459.9 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from ca-core-news-trf==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-transformers<1.3.0,>=1.2.0.dev0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from ca-core-news-trf==3.5.0) (1.2.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (65.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: transformers<4.30.0,>=3.4.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (4.29.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (2.0.1)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (0.13.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ca-core-news-trf==3.5.0) (2.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (2023.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\adria\\ia\\4t quadri\\plh\\env\\lib\\site-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->ca-core-news-trf==3.5.0) (1.3.0)\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('ca_core_news_trf')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Imports necessaris\n",
    "!py -m spacy download ca_core_news_md\n",
    "!py -m spacy download ca_core_news_trf\n",
    "\n",
    "##!python3 -m pip install spacy-transformers\n",
    "##!python3 -m pip install sentence_transformers\n",
    "\n",
    "# Requisitos\n",
    "\n",
    "from gensim.models import TfidfModel, KeyedVectors, fasttext\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "# Tipado\n",
    "from typing import Tuple, List\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from scipy.stats import pearsonr\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "jPqAOO7h8m7l",
    "outputId": "a7b96f2c-6c12-409f-908d-e75c5c31df32"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cargar stopwords en Catalan\n",
    "# STOPWORDS_CA = {\"a\", \"abans\", \"ací\", \"ah\", \"així\", \"això\", \"al\", \"aleshores\", \"algun\", \"alguna\", \"algunes\", \"alguns\", \"alhora\", \"allà\", \"allí\", \"allò\", \"als\", \"altra\", \"altre\", \"altres\", \"amb\", \"ambdues\", \"ambdós\", \"anar\", \"ans\", \"apa\", \"aquell\", \"aquella\", \"aquelles\", \"aquells\", \"aquest\", \"aquesta\", \"aquestes\", \"aquests\", \"aquí\", \"baix\", \"bastant\", \"bé\", \"cada\", \"cadascuna\", \"cadascunes\", \"cadascuns\", \"cadascú\", \"com\", \"consegueixo\", \"conseguim\", \"conseguir\", \"consigueix\", \"consigueixen\", \"consigueixes\", \"contra\", \"d'un\", \"d'una\", \"d'unes\", \"d'uns\", \"dalt\", \"de\", \"del\", \"dels\", \"des\", \"des de\", \"després\", \"dins\", \"dintre\", \"donat\", \"doncs\", \"durant\", \"e\", \"eh\", \"el\", \"elles\", \"ells\", \"els\", \"em\", \"en\", \"encara\", \"ens\", \"entre\", \"era\", \"erem\", \"eren\", \"eres\", \"es\", \"esta\", \"estan\", \"estat\", \"estava\", \"estaven\", \"estem\", \"esteu\", \"estic\", \"està\", \"estàvem\", \"estàveu\", \"et\", \"etc\", \"ets\", \"fa\", \"faig\", \"fan\", \"fas\", \"fem\", \"fer\", \"feu\", \"fi\", \"fins\", \"fora\", \"gairebé\", \"ha\", \"han\", \"has\", \"haver\", \"havia\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \"i\", \"igual\", \"iguals\", \"inclòs\", \"ja\", \"jo\", \"l'hi\", \"la\", \"les\", \"li\", \"li'n\", \"llarg\", \"llavors\", \"m'he\", \"ma\", \"mal\", \"malgrat\", \"mateix\", \"mateixa\", \"mateixes\", \"mateixos\", \"me\", \"mentre\", \"meu\", \"meus\", \"meva\", \"meves\", \"mode\", \"molt\", \"molta\", \"moltes\", \"molts\", \"mon\", \"mons\", \"més\", \"n'he\", \"n'hi\", \"ne\", \"ni\", \"no\", \"nogensmenys\", \"només\", \"nosaltres\", \"nostra\", \"nostre\", \"nostres\", \"o\", \"oh\", \"oi\", \"on\", \"pas\", \"pel\", \"pels\", \"per\", \"per que\", \"perquè\", \"però\", \"poc\", \"poca\", \"pocs\", \"podem\", \"poden\", \"poder\", \"podeu\", \"poques\", \"potser\", \"primer\", \"propi\", \"puc\", \"qual\", \"quals\", \"quan\", \"quant\", \"que\", \"quelcom\", \"qui\", \"quin\", \"quina\", \"quines\", \"quins\", \"què\", \"s'ha\", \"s'han\", \"sa\", \"sabem\", \"saben\", \"saber\", \"sabeu\", \"sap\", \"saps\", \"semblant\", \"semblants\", \"sense\", \"ser\", \"ses\", \"seu\", \"seus\", \"seva\", \"seves\", \"si\", \"sobre\", \"sobretot\", \"soc\", \"solament\", \"sols\", \"som\", \"son\", \"sons\", \"sota\", \"sou\", \"sóc\", \"són\", \"t'ha\", \"t'han\", \"t'he\", \"ta\", \"tal\", \"també\", \"tampoc\", \"tan\", \"tant\", \"tanta\", \"tantes\", \"te\", \"tene\", \"tenim\", \"tenir\", \"teniu\", \"teu\", \"teus\", \"teva\", \"teves\", \"tinc\", \"ton\", \"tons\", \"tot\", \"tota\", \"totes\", \"tots\", \"un\", \"una\", \"unes\", \"uns\", \"us\", \"va\", \"vaig\", \"vam\", \"van\", \"vas\", \"veu\", \"vosaltres\", \"vostra\", \"vostre\", \"vostres\", \"érem\", \"éreu\", \"és\", \"éssent\", \"últim\", \"ús\"}\n",
    "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
    "\n",
    "# Definir función de pre-procesado\n",
    "def preprocess(sentence: str) -> List[str]:\n",
    "    preprocessed = simple_preprocess(sentence)\n",
    "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
    "    return preprocessed\n",
    "\n",
    "## Introducir los datos de train y de validación\n",
    "input_pairs = list(zip(dataset[\"train\"][\"sentence1\"], dataset[\"train\"][\"sentence2\"], dataset[\"train\"][\"label\"]))\n",
    "input_pairs_val = list(zip(dataset[\"validation\"][\"sentence1\"], dataset[\"validation\"][\"sentence2\"], dataset[\"validation\"][\"label\"]))\n",
    "\n",
    "\n",
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [preprocess(sentence_1) for sentence_1, _, _ in input_pairs]\n",
    "sentences_2_preproc = [preprocess(sentence_2) for _, sentence_2, _ in input_pairs]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "\n",
    "diccionario = Dictionary(sentences_pairs_flattened)\n",
    "\n",
    "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
    "modelo_tfidf = TfidfModel(corpus)\n",
    "\n",
    "def creacio_one_hot(sentence, diccionario:Dictionary):\n",
    "    diccionario.filter_extremes(no_below=5, no_above=0.5, keep_n=300)\n",
    "    vector1 = np.zeros(len(diccionario.token2id), dtype=np.float)\n",
    "    bow1 = diccionario.doc2bow(sentence)\n",
    "    for index, count in bow1:\n",
    "        vector1[index] = count\n",
    "\n",
    "    return vector1\n",
    "\n",
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    bow = dictionary.doc2bow(sentence_preproc)\n",
    "    tf_idf = tf_idf_model[bow]\n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in wv_model:\n",
    "            vectors.append(wv_model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        dictionary: Dictionary = None,\n",
    "        tf_idf_model: TfidfModel = None,\n",
    "        one_hot: bool = None,\n",
    "        Spasii : bool = None,\n",
    "        RobertA: bool = None,\n",
    "        RobertA_Mean: bool = None,\n",
    "        wv100: bool = None,\n",
    "        wv500: bool = None,\n",
    "        w2v_pre_mean: bool = None,\n",
    "        RobertA_finetuned: bool = None\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
    "    :param sentence_pairs:\n",
    "    :param dictionary:\n",
    "    :param tf_idf_model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if one_hot:\n",
    "        dictionary2 = Dictionary(sentences_pairs_flattened, prune_at=300)\n",
    "\n",
    "    if Spasii:\n",
    "        nlp = spacy.load(\"ca_core_news_md\")\n",
    "    if RobertA or RobertA_Mean:\n",
    "        nlp = spacy.load(\"ca_core_news_trf\")\n",
    "\n",
    "    if wv100:\n",
    "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec100.bin\", binary = True)\n",
    "\n",
    "    if wv500:\n",
    "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec500.bin\", binary = True)\n",
    "\n",
    "    if w2v_pre_mean or tf_idf_model:\n",
    "        wv_model = KeyedVectors.load_word2vec_format(\"data/model.bin\", binary = True)\n",
    "\n",
    "    if RobertA_finetuned:\n",
    "        model_name = 'stsb-roberta-base'\n",
    "        model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess(sentence_1)\n",
    "        sentence_2_preproc = preprocess(sentence_2)\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, wv_model=wv_model )\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model,wv_model=wv_model )\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "\n",
    "        elif wv100 is not None:\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "\n",
    "        elif wv500 is not None:\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "\n",
    "        elif one_hot is not None:\n",
    "            vector1 = creacio_one_hot(sentence=sentence_1_preproc, diccionario=dictionary2)\n",
    "            vector2 = creacio_one_hot(sentence=sentence_2_preproc, diccionario= dictionary2)\n",
    "            \n",
    "        elif Spasii is not None:\n",
    "            vector1 = nlp(\" \".join(sentence_1_preproc)).vector\n",
    "            vector2 = nlp(\" \".join(sentence_2_preproc)).vector\n",
    "\n",
    "        elif RobertA is not None:\n",
    "            vector1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
    "            vector2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
    "\n",
    "        elif RobertA_Mean is not None:\n",
    "            vectors1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
    "            vectors2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
    "\n",
    "            vector1 = np.average(vectors1, axis=0)\n",
    "            vector2 = np.average(vectors2, axis=0)\n",
    "\n",
    "        elif RobertA_finetuned is not None:\n",
    "            vector1 = model.encode([\" \".join(sentence_1_preproc)])[0]\n",
    "            vector2 = model.encode([\" \".join(sentence_2_preproc)])[0]\n",
    "\n",
    "\n",
    "            \n",
    "        elif w2v_pre_mean is not None:\n",
    "            # Cálculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
    "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqBiQ_7R8m7l"
   },
   "source": [
    "Fem la funció de la creació del model y dels hiperparámetres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KyNL-8mQ8m7m"
   },
   "outputs": [],
   "source": [
    "def build_and_compile_model(hidden_size: int = 128, embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
    "    # Capa de entrada para los pares de vectores\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
    "\n",
    "    # Capa oculta\n",
    "    first_projection = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        # activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "    )\n",
    "    projected_1 = first_projection(input_1)\n",
    "    projected_2 = first_projection(input_2)\n",
    "\n",
    "    # Compute the cosine distance\n",
    "    projected_1 = tf.linalg.l2_normalize(projected_1, axis=1, )\n",
    "    projected_2 = tf.linalg.l2_normalize(projected_2, axis=1, )\n",
    "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
    "\n",
    "    # Definir el modelo con las capas de entrada y salida\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definir constantes de entrenamiento\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64\n",
    "\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
    "    :param pair_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )\n",
    "\n",
    "\n",
    "def create_and_train_and_evaluate_model(mapped, mapped_val,embedding_size:int=300):\n",
    "    # Obtener las listas de train y test\n",
    "    x_train, y_train = pair_list_to_x_y(mapped)\n",
    "    x_val, y_val = pair_list_to_x_y(mapped_val)\n",
    "\n",
    "    # Preparar los conjuntos de datos de entrenamiento y validación\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    # Construir y compilar el modelo\n",
    "    model = build_and_compile_model(embedding_size=embedding_size)\n",
    "    # tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, )\n",
    "    print(model.summary())\n",
    "    # Entrenar el modelo\n",
    "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
    "\n",
    "    y_pred: tf.RaggedTensor = model.predict(x_val)\n",
    "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "    correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "    # Imprimir el coeficiente de correlación de Pearson\n",
    "    print(f\"Correlación de Pearson: {correlation}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zvHCfM7_8m7m"
   },
   "source": [
    "### Compare results with different word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqZtqXFR8m7m"
   },
   "source": [
    "#### 0. Model de Word2Vec entrenats per nosaltres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A3IweZK8m7m"
   },
   "source": [
    "##### 0.1. Model amb 100 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZENU8kuc8m7m"
   },
   "outputs": [],
   "source": [
    "mapped = map_pairs(input_pairs, wv100=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, wv100=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1n7zMRP38m7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_4 (TFOpLa  (None, 100)         0           ['dense_2[0][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_5 (TFOpLa  (None, 100)         0           ['dense_2[1][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 100)         0           ['tf.math.l2_normalize_4[0][0]', \n",
      " )                                                                'tf.math.l2_normalize_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_2 (TFOpLamb  (None,)             0           ['tf.math.multiply_4[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None,)             0           ['tf.math.reduce_sum_2[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None,)             0           ['tf.__operators__.add_2[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,100\n",
      "Trainable params: 10,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 3s 9ms/step - loss: 1.9693 - val_loss: 1.8219\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7980 - val_loss: 1.7359\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7127 - val_loss: 1.6809\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6536 - val_loss: 1.6444\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6086 - val_loss: 1.6171\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5714 - val_loss: 1.5952\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 1.5773\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5115 - val_loss: 1.5636\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4873 - val_loss: 1.5519\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4652 - val_loss: 1.5426\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4461 - val_loss: 1.5352\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4276 - val_loss: 1.5279\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4113 - val_loss: 1.5231\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3964 - val_loss: 1.5173\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3821 - val_loss: 1.5123\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3697 - val_loss: 1.5081\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3569 - val_loss: 1.5041\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3447 - val_loss: 1.5024\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3353 - val_loss: 1.4977\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3240 - val_loss: 1.4941\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3139 - val_loss: 1.4913\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3059 - val_loss: 1.4918\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2975 - val_loss: 1.4867\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2876 - val_loss: 1.4847\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2815 - val_loss: 1.4828\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2738 - val_loss: 1.4821\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2667 - val_loss: 1.4787\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2601 - val_loss: 1.4775\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2532 - val_loss: 1.4752\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2474 - val_loss: 1.4766\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2427 - val_loss: 1.4721\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2368 - val_loss: 1.4706\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2313 - val_loss: 1.4698\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2263 - val_loss: 1.4706\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2221 - val_loss: 1.4664\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2171 - val_loss: 1.4668\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2140 - val_loss: 1.4660\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2087 - val_loss: 1.4660\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2057 - val_loss: 1.4643\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2015 - val_loss: 1.4643\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1965 - val_loss: 1.4633\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1930 - val_loss: 1.4633\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1918 - val_loss: 1.4599\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1881 - val_loss: 1.4616\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1836 - val_loss: 1.4622\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1818 - val_loss: 1.4574\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1791 - val_loss: 1.4626\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1786 - val_loss: 1.4546\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1743 - val_loss: 1.4564\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1709 - val_loss: 1.4580\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1695 - val_loss: 1.4577\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1682 - val_loss: 1.4554\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1656 - val_loss: 1.4563\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1629 - val_loss: 1.4546\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1611 - val_loss: 1.4540\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1586 - val_loss: 1.4520\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1576 - val_loss: 1.4559\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1566 - val_loss: 1.4497\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1520 - val_loss: 1.4517\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1536 - val_loss: 1.4543\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1530 - val_loss: 1.4502\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1468 - val_loss: 1.4478\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1463 - val_loss: 1.4486\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1459 - val_loss: 1.4511\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.4454770319025318\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val,embedding_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW-pSYQe8m7n"
   },
   "source": [
    "##### 0.2. Model amb 500 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xxx6u1Ig8m7n"
   },
   "outputs": [],
   "source": [
    "mapped = map_pairs(input_pairs, wv500=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, wv500=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SmOYMJ048m7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          10100       ['input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_8 (TFOpLa  (None, 100)         0           ['dense_4[0][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_9 (TFOpLa  (None, 100)         0           ['dense_4[1][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 100)         0           ['tf.math.l2_normalize_8[0][0]', \n",
      " )                                                                'tf.math.l2_normalize_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_4 (TFOpLamb  (None,)             0           ['tf.math.multiply_8[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None,)             0           ['tf.math.reduce_sum_4[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None,)             0           ['tf.__operators__.add_4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,100\n",
      "Trainable params: 10,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 3s 7ms/step - loss: 2.0262 - val_loss: 1.8946\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.8694 - val_loss: 1.8067\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7836 - val_loss: 1.7461\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7201 - val_loss: 1.7022\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6694 - val_loss: 1.6678\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6265 - val_loss: 1.6380\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5902 - val_loss: 1.6148\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5586 - val_loss: 1.5957\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5320 - val_loss: 1.5822\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5083 - val_loss: 1.5718\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4874 - val_loss: 1.5625\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4678 - val_loss: 1.5526\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4507 - val_loss: 1.5456\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4347 - val_loss: 1.5383\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4196 - val_loss: 1.5324\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4059 - val_loss: 1.5275\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3934 - val_loss: 1.5236\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3822 - val_loss: 1.5195\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3714 - val_loss: 1.5173\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3612 - val_loss: 1.5144\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3514 - val_loss: 1.5091\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3431 - val_loss: 1.5095\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 1.5042\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3263 - val_loss: 1.5018\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3195 - val_loss: 1.5013\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3105 - val_loss: 1.4992\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3045 - val_loss: 1.4938\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2983 - val_loss: 1.4928\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2911 - val_loss: 1.4914\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2854 - val_loss: 1.4874\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2798 - val_loss: 1.4868\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2735 - val_loss: 1.4844\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2689 - val_loss: 1.4833\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2625 - val_loss: 1.4815\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2573 - val_loss: 1.4783\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2541 - val_loss: 1.4761\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2487 - val_loss: 1.4751\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2428 - val_loss: 1.4756\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2395 - val_loss: 1.4730\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2352 - val_loss: 1.4706\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2314 - val_loss: 1.4705\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2263 - val_loss: 1.4686\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2243 - val_loss: 1.4717\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 1.4647\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2174 - val_loss: 1.4643\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2134 - val_loss: 1.4635\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2119 - val_loss: 1.4640\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2066 - val_loss: 1.4632\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2061 - val_loss: 1.4618\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2053 - val_loss: 1.4594\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1990 - val_loss: 1.4603\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1981 - val_loss: 1.4603\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1955 - val_loss: 1.4603\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1942 - val_loss: 1.4558\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1886 - val_loss: 1.4569\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1883 - val_loss: 1.4565\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1856 - val_loss: 1.4572\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1836 - val_loss: 1.4562\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1832 - val_loss: 1.4516\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1771 - val_loss: 1.4582\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1802 - val_loss: 1.4545\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1759 - val_loss: 1.4556\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1742 - val_loss: 1.4567\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1761 - val_loss: 1.4537\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.36222415102232647\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val,embedding_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QF0EG-AD8m7n"
   },
   "source": [
    "#### 1. One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7Z55O8jG8m7n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_15488\\1179692162.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vector1 = np.zeros(len(diccionario.token2id), dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "mapped = map_pairs(input_pairs, one_hot=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, one_hot=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "JoNQhiIg8m7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 300)          90300       ['input_21[0][0]',               \n",
      "                                                                  'input_22[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_20 (TFOpL  (None, 300)         0           ['dense_10[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_21 (TFOpL  (None, 300)         0           ['dense_10[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpLambd  (None, 300)         0           ['tf.math.l2_normalize_20[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_21[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_10 (TFOpLam  (None,)             0           ['tf.math.multiply_20[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None,)             0           ['tf.math.reduce_sum_10[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpLambd  (None,)             0           ['tf.__operators__.add_10[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90,300\n",
      "Trainable params: 90,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 2s 9ms/step - loss: 1.5401 - val_loss: 1.4911\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4658 - val_loss: 1.4614\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3944 - val_loss: 1.4348\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3217 - val_loss: 1.4099\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2566 - val_loss: 1.3926\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2025 - val_loss: 1.3794\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1548 - val_loss: 1.3681\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1139 - val_loss: 1.3598\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0797 - val_loss: 1.3528\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0493 - val_loss: 1.3484\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0235 - val_loss: 1.3432\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0004 - val_loss: 1.3399\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9811 - val_loss: 1.3377\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9621 - val_loss: 1.3337\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9459 - val_loss: 1.3317\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9315 - val_loss: 1.3306\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9174 - val_loss: 1.3287\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9059 - val_loss: 1.3280\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8935 - val_loss: 1.3283\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8824 - val_loss: 1.3276\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8729 - val_loss: 1.3252\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8642 - val_loss: 1.3269\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8557 - val_loss: 1.3256\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8481 - val_loss: 1.3262\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8387 - val_loss: 1.3275\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8323 - val_loss: 1.3280\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8258 - val_loss: 1.3293\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8192 - val_loss: 1.3278\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8128 - val_loss: 1.3308\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8080 - val_loss: 1.3302\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8013 - val_loss: 1.3317\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7978 - val_loss: 1.3327\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7930 - val_loss: 1.3324\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7868 - val_loss: 1.3331\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7818 - val_loss: 1.3318\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7802 - val_loss: 1.3315\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7747 - val_loss: 1.3321\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7707 - val_loss: 1.3335\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7675 - val_loss: 1.3332\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7644 - val_loss: 1.3333\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7592 - val_loss: 1.3314\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7575 - val_loss: 1.3338\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7534 - val_loss: 1.3341\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7502 - val_loss: 1.3343\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7479 - val_loss: 1.3329\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7447 - val_loss: 1.3315\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7420 - val_loss: 1.3333\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7397 - val_loss: 1.3352\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7352 - val_loss: 1.3334\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 1.3345\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7318 - val_loss: 1.3341\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7291 - val_loss: 1.3340\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7284 - val_loss: 1.3360\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7262 - val_loss: 1.3356\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7235 - val_loss: 1.3365\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7208 - val_loss: 1.3351\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7192 - val_loss: 1.3346\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7184 - val_loss: 1.3336\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7151 - val_loss: 1.3346\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7150 - val_loss: 1.3345\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 1.3346\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7115 - val_loss: 1.3354\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7100 - val_loss: 1.3343\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7080 - val_loss: 1.3346\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.28833829882164796\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dmfqVPvY8m7n"
   },
   "source": [
    "#### 2. Models de Word2Vec/GloVe pre-entrenats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FGELWVb88m7n"
   },
   "source": [
    "##### 2.1.Word2Vec + Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kvjn7pJL8m7o"
   },
   "outputs": [],
   "source": [
    "mapped = map_pairs(input_pairs, w2v_pre_mean=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, w2v_pre_mean=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "rHRxNJ_J8m7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 100)          10100       ['input_25[0][0]',               \n",
      "                                                                  'input_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_24 (TFOpL  (None, 100)         0           ['dense_12[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_25 (TFOpL  (None, 100)         0           ['dense_12[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None, 100)         0           ['tf.math.l2_normalize_24[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_25[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_12 (TFOpLam  (None,)             0           ['tf.math.multiply_24[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None,)             0           ['tf.math.reduce_sum_12[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_25 (TFOpLambd  (None,)             0           ['tf.__operators__.add_12[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,100\n",
      "Trainable params: 10,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 2s 7ms/step - loss: 2.0923 - val_loss: 1.9562\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.9313 - val_loss: 1.8784\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.8531 - val_loss: 1.8273\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7970 - val_loss: 1.7909\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7530 - val_loss: 1.7629\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7162 - val_loss: 1.7399\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6844 - val_loss: 1.7202\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6561 - val_loss: 1.7040\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6309 - val_loss: 1.6889\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6070 - val_loss: 1.6777\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5871 - val_loss: 1.6659\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5684 - val_loss: 1.6586\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5528 - val_loss: 1.6495\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5370 - val_loss: 1.6447\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5232 - val_loss: 1.6371\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5106 - val_loss: 1.6325\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4982 - val_loss: 1.6271\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4870 - val_loss: 1.6218\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4766 - val_loss: 1.6189\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4674 - val_loss: 1.6153\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4571 - val_loss: 1.6103\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4493 - val_loss: 1.6089\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4395 - val_loss: 1.6048\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4332 - val_loss: 1.6030\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4267 - val_loss: 1.5996\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4178 - val_loss: 1.5967\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4120 - val_loss: 1.5954\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4049 - val_loss: 1.5932\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3989 - val_loss: 1.5900\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3946 - val_loss: 1.5875\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3879 - val_loss: 1.5860\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3817 - val_loss: 1.5826\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3773 - val_loss: 1.5828\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3746 - val_loss: 1.5808\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3672 - val_loss: 1.5800\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3641 - val_loss: 1.5782\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3607 - val_loss: 1.5748\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3552 - val_loss: 1.5751\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3512 - val_loss: 1.5744\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3480 - val_loss: 1.5723\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3445 - val_loss: 1.5722\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3399 - val_loss: 1.5723\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3377 - val_loss: 1.5689\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3334 - val_loss: 1.5677\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3296 - val_loss: 1.5693\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3270 - val_loss: 1.5674\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3240 - val_loss: 1.5639\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3205 - val_loss: 1.5642\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3192 - val_loss: 1.5655\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3174 - val_loss: 1.5678\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3127 - val_loss: 1.5616\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3111 - val_loss: 1.5619\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3082 - val_loss: 1.5638\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3052 - val_loss: 1.5633\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3050 - val_loss: 1.5601\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3008 - val_loss: 1.5604\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2983 - val_loss: 1.5593\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2954 - val_loss: 1.5601\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2918 - val_loss: 1.5583\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2902 - val_loss: 1.5584\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2899 - val_loss: 1.5595\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2888 - val_loss: 1.5578\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2829 - val_loss: 1.5553\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2828 - val_loss: 1.5570\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.31542518139487363\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val, embedding_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oDA0wr-X8m7o"
   },
   "source": [
    "##### 2.2.Word2Vec + Mean ponderada (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "g8zH2ip08m7o"
   },
   "outputs": [],
   "source": [
    "mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "uSMyClBN8m7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 100)          10100       ['input_27[0][0]',               \n",
      "                                                                  'input_28[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_26 (TFOpL  (None, 100)         0           ['dense_13[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_27 (TFOpL  (None, 100)         0           ['dense_13[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_26 (TFOpLambd  (None, 100)         0           ['tf.math.l2_normalize_26[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_27[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_13 (TFOpLam  (None,)             0           ['tf.math.multiply_26[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None,)             0           ['tf.math.reduce_sum_13[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_27 (TFOpLambd  (None,)             0           ['tf.__operators__.add_13[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,100\n",
      "Trainable params: 10,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 1s 10ms/step - loss: 2.0572 - val_loss: 1.8973\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8941 - val_loss: 1.8261\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8150 - val_loss: 1.7818\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7593 - val_loss: 1.7511\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7164 - val_loss: 1.7272\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6813 - val_loss: 1.7080\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6513 - val_loss: 1.6911\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6248 - val_loss: 1.6775\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6007 - val_loss: 1.6658\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5783 - val_loss: 1.6552\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5593 - val_loss: 1.6477\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5422 - val_loss: 1.6400\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5267 - val_loss: 1.6345\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5133 - val_loss: 1.6289\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5004 - val_loss: 1.6253\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4886 - val_loss: 1.6212\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4782 - val_loss: 1.6153\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4680 - val_loss: 1.6147\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4586 - val_loss: 1.6115\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4497 - val_loss: 1.6070\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4401 - val_loss: 1.6045\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4325 - val_loss: 1.6048\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4245 - val_loss: 1.5999\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4166 - val_loss: 1.5983\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4102 - val_loss: 1.5959\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4029 - val_loss: 1.5968\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3963 - val_loss: 1.5929\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3897 - val_loss: 1.5922\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3839 - val_loss: 1.5870\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3783 - val_loss: 1.5882\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3714 - val_loss: 1.5875\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3666 - val_loss: 1.5851\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3627 - val_loss: 1.5850\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3562 - val_loss: 1.5820\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3526 - val_loss: 1.5823\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3478 - val_loss: 1.5838\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3425 - val_loss: 1.5794\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3387 - val_loss: 1.5796\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3353 - val_loss: 1.5804\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3298 - val_loss: 1.5800\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3259 - val_loss: 1.5777\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3226 - val_loss: 1.5748\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3195 - val_loss: 1.5774\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3164 - val_loss: 1.5762\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3122 - val_loss: 1.5767\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3075 - val_loss: 1.5733\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3057 - val_loss: 1.5749\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3034 - val_loss: 1.5746\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3005 - val_loss: 1.5719\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2954 - val_loss: 1.5706\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2957 - val_loss: 1.5723\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2907 - val_loss: 1.5750\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2881 - val_loss: 1.5721\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2864 - val_loss: 1.5689\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2830 - val_loss: 1.5689\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2797 - val_loss: 1.5680\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2784 - val_loss: 1.5706\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2743 - val_loss: 1.5675\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2752 - val_loss: 1.5725\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2719 - val_loss: 1.5675\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2688 - val_loss: 1.5674\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2676 - val_loss: 1.5677\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2643 - val_loss: 1.5648\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2625 - val_loss: 1.5676\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.2993364135088163\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val, embedding_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GsHl1GQD8m72"
   },
   "source": [
    "#### 3.Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Asd1Oecv8m72"
   },
   "source": [
    "Primer definim els embeddings amb el model de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "x-Q4cmr38m72"
   },
   "outputs": [],
   "source": [
    "mapped = map_pairs(input_pairs, Spasii=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, Spasii=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "juK2JoYz8m72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 300)          90300       ['input_29[0][0]',               \n",
      "                                                                  'input_30[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_28 (TFOpL  (None, 300)         0           ['dense_14[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_29 (TFOpL  (None, 300)         0           ['dense_14[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_28 (TFOpLambd  (None, 300)         0           ['tf.math.l2_normalize_28[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_29[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_14 (TFOpLam  (None,)             0           ['tf.math.multiply_28[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None,)             0           ['tf.math.reduce_sum_14[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_29 (TFOpLambd  (None,)             0           ['tf.__operators__.add_14[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90,300\n",
      "Trainable params: 90,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 1s 9ms/step - loss: 1.8001 - val_loss: 1.5949\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6014 - val_loss: 1.5363\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5308 - val_loss: 1.5088\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4850 - val_loss: 1.4917\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4482 - val_loss: 1.4768\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4186 - val_loss: 1.4707\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3906 - val_loss: 1.4605\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3660 - val_loss: 1.4516\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3442 - val_loss: 1.4507\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3232 - val_loss: 1.4377\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3028 - val_loss: 1.4396\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2849 - val_loss: 1.4331\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2724 - val_loss: 1.4374\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2553 - val_loss: 1.4337\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2441 - val_loss: 1.4346\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2348 - val_loss: 1.4291\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2183 - val_loss: 1.4273\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2022 - val_loss: 1.4237\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1880 - val_loss: 1.4280\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1829 - val_loss: 1.4287\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1755 - val_loss: 1.4168\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1584 - val_loss: 1.4280\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1540 - val_loss: 1.4216\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1444 - val_loss: 1.4216\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1389 - val_loss: 1.4169\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1249 - val_loss: 1.4284\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1264 - val_loss: 1.4113\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1184 - val_loss: 1.4184\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1142 - val_loss: 1.4286\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1117 - val_loss: 1.4132\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0980 - val_loss: 1.4079\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0921 - val_loss: 1.4196\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0818 - val_loss: 1.4115\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0762 - val_loss: 1.4137\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0689 - val_loss: 1.4073\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0683 - val_loss: 1.4029\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0561 - val_loss: 1.4114\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0530 - val_loss: 1.4095\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0477 - val_loss: 1.4054\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0366 - val_loss: 1.3966\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0436 - val_loss: 1.4160\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0397 - val_loss: 1.4108\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0248 - val_loss: 1.4085\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0208 - val_loss: 1.3995\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0204 - val_loss: 1.4047\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0171 - val_loss: 1.4125\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0090 - val_loss: 1.4007\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0055 - val_loss: 1.3955\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0042 - val_loss: 1.4115\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0051 - val_loss: 1.4099\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0037 - val_loss: 1.3973\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9911 - val_loss: 1.3984\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9866 - val_loss: 1.4094\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9848 - val_loss: 1.4095\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9865 - val_loss: 1.4011\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9819 - val_loss: 1.3976\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9862 - val_loss: 1.3962\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9776 - val_loss: 1.4022\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9691 - val_loss: 1.3969\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9613 - val_loss: 1.3933\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9614 - val_loss: 1.3932\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9644 - val_loss: 1.3997\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9641 - val_loss: 1.3962\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9503 - val_loss: 1.3928\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "Correlación de Pearson: 0.2831527264667054\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SkeQ57oO8m72"
   },
   "source": [
    "#### 4.RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CBGLTSu28m73"
   },
   "source": [
    "##### 4.1.CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "GjFoNCWB8m73"
   },
   "outputs": [],
   "source": [
    "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
    "mapped = map_pairs(input_pairs, RobertA=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, RobertA=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ClT4pyhg8m73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 300)          90300       ['input_31[0][0]',               \n",
      "                                                                  'input_32[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_30 (TFOpL  (None, 300)         0           ['dense_15[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_31 (TFOpL  (None, 300)         0           ['dense_15[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_30 (TFOpLambd  (None, 300)         0           ['tf.math.l2_normalize_30[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_31[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_15 (TFOpLam  (None,)             0           ['tf.math.multiply_30[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None,)             0           ['tf.math.reduce_sum_15[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_31 (TFOpLambd  (None,)             0           ['tf.__operators__.add_15[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90,300\n",
      "Trainable params: 90,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_15\" is incompatible with the layer: expected shape=(None, 300), found shape=(None, 1, 768)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_and_train_and_evaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmapped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapped_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 62\u001B[0m, in \u001B[0;36mcreate_and_train_and_evaluate_model\u001B[1;34m(mapped, mapped_val, embedding_size)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39msummary())\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Entrenar el modelo\u001B[39;00m\n\u001B[1;32m---> 62\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m y_pred: tf\u001B[38;5;241m.\u001B[39mRaggedTensor \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(x_val)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\u001B[39;00m\n",
      "File \u001B[1;32m~\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetb0i3zl9.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\adria\\IA\\4t Quadri\\PLH\\env\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_15\" is incompatible with the layer: expected shape=(None, 300), found shape=(None, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "d5qL6zGN8m73"
   },
   "source": [
    "##### 4.2.MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "aCxEI7aH8m73"
   },
   "outputs": [],
   "source": [
    "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
    "mapped = map_pairs(input_pairs, RobertA_Mean=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, RobertA_Mean=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "AaDmE7D88m73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 768)          590592      ['input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_34 (TFOpL  (None, 768)         0           ['dense_17[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_35 (TFOpL  (None, 768)         0           ['dense_17[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_34 (TFOpLambd  (None, 768)         0           ['tf.math.l2_normalize_34[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_35[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_17 (TFOpLam  (None,)             0           ['tf.math.multiply_34[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None,)             0           ['tf.math.reduce_sum_17[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_35 (TFOpLambd  (None,)             0           ['tf.__operators__.add_17[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 2s 11ms/step - loss: 1.5027 - val_loss: 1.3032\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.3158 - val_loss: 1.2767\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.2489 - val_loss: 1.2157\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.1891 - val_loss: 1.2310\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.1685 - val_loss: 1.1946\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.1325 - val_loss: 1.1896\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.0941 - val_loss: 1.1945\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0708 - val_loss: 1.1964\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0562 - val_loss: 1.1699\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0338 - val_loss: 1.1452\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0252 - val_loss: 1.1594\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0217 - val_loss: 1.1912\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1.0031 - val_loss: 1.1487\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9774 - val_loss: 1.1709\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.9781 - val_loss: 1.1850\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.9701 - val_loss: 1.1476\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9494 - val_loss: 1.1273\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9428 - val_loss: 1.1450\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9253 - val_loss: 1.1316\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9214 - val_loss: 1.1237\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9118 - val_loss: 1.1250\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.9053 - val_loss: 1.1212\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8952 - val_loss: 1.1466\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8827 - val_loss: 1.1031\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8651 - val_loss: 1.1058\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8663 - val_loss: 1.1129\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8589 - val_loss: 1.1260\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8502 - val_loss: 1.1198\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8575 - val_loss: 1.1241\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8525 - val_loss: 1.1077\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8303 - val_loss: 1.0879\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8205 - val_loss: 1.1158\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8313 - val_loss: 1.0890\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8227 - val_loss: 1.0970\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8214 - val_loss: 1.0955\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.8259 - val_loss: 1.0879\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8272 - val_loss: 1.0993\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8072 - val_loss: 1.1062\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8016 - val_loss: 1.0961\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8118 - val_loss: 1.0954\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7955 - val_loss: 1.0938\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7949 - val_loss: 1.0881\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7968 - val_loss: 1.0958\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7891 - val_loss: 1.1308\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8053 - val_loss: 1.1124\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7964 - val_loss: 1.1084\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7955 - val_loss: 1.1218\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8082 - val_loss: 1.1281\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8102 - val_loss: 1.1304\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8190 - val_loss: 1.1310\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8216 - val_loss: 1.1256\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8179 - val_loss: 1.0748\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7926 - val_loss: 1.0711\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7773 - val_loss: 1.0644\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7674 - val_loss: 1.0564\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7459 - val_loss: 1.0710\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7594 - val_loss: 1.0747\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7370 - val_loss: 1.0862\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.7417 - val_loss: 1.0691\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7300 - val_loss: 1.0525\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7254 - val_loss: 1.1015\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7276 - val_loss: 1.1009\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7248 - val_loss: 1.0526\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7145 - val_loss: 1.0549\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "Correlación de Pearson: 0.20825092794828465\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val,embedding_size=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9GYDsChM8m73"
   },
   "source": [
    "#### 5.RoBERTa FineTuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:48:42.264845Z",
     "start_time": "2023-05-31T17:11:49.470503Z"
    },
    "id": "OZd_7rU-8m74"
   },
   "outputs": [],
   "source": [
    "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
    "mapped = map_pairs(input_pairs, RobertA_finetuned=True, dictionary=diccionario )\n",
    "mapped_val = map_pairs(input_pairs_val, RobertA_finetuned=True, dictionary=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "LnrRULCk8m74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 768)          590592      ['input_39[0][0]',               \n",
      "                                                                  'input_40[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_38 (TFOpL  (None, 768)         0           ['dense_19[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_39 (TFOpL  (None, 768)         0           ['dense_19[1][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_38 (TFOpLambd  (None, 768)         0           ['tf.math.l2_normalize_38[0][0]',\n",
      " a)                                                               'tf.math.l2_normalize_39[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_19 (TFOpLam  (None,)             0           ['tf.math.multiply_38[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None,)             0           ['tf.math.reduce_sum_19[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_39 (TFOpLambd  (None,)             0           ['tf.__operators__.add_19[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "33/33 [==============================] - 1s 12ms/step - loss: 1.3816 - val_loss: 1.2529\n",
      "Epoch 2/64\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 1.1695 - val_loss: 1.2335\n",
      "Epoch 3/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 1.0761 - val_loss: 1.2230\n",
      "Epoch 4/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 1.0072 - val_loss: 1.2227\n",
      "Epoch 5/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.9531 - val_loss: 1.2097\n",
      "Epoch 6/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.9021 - val_loss: 1.2008\n",
      "Epoch 7/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.8614 - val_loss: 1.1927\n",
      "Epoch 8/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.8278 - val_loss: 1.1960\n",
      "Epoch 9/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.7898 - val_loss: 1.1821\n",
      "Epoch 10/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.7623 - val_loss: 1.2032\n",
      "Epoch 11/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.7358 - val_loss: 1.1791\n",
      "Epoch 12/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.7075 - val_loss: 1.1707\n",
      "Epoch 13/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.6840 - val_loss: 1.1747\n",
      "Epoch 14/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.6731 - val_loss: 1.1997\n",
      "Epoch 15/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.6589 - val_loss: 1.1647\n",
      "Epoch 16/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.6211 - val_loss: 1.1689\n",
      "Epoch 17/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.6073 - val_loss: 1.1633\n",
      "Epoch 18/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5990 - val_loss: 1.1603\n",
      "Epoch 19/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.5794 - val_loss: 1.1833\n",
      "Epoch 20/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.5688 - val_loss: 1.1690\n",
      "Epoch 21/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.5527 - val_loss: 1.1563\n",
      "Epoch 22/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.5391 - val_loss: 1.1617\n",
      "Epoch 23/64\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 0.5254 - val_loss: 1.1735\n",
      "Epoch 24/64\n",
      "33/33 [==============================] - 0s 13ms/step - loss: 0.5149 - val_loss: 1.1648\n",
      "Epoch 25/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.5051 - val_loss: 1.1724\n",
      "Epoch 26/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.5066 - val_loss: 1.1507\n",
      "Epoch 27/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.4893 - val_loss: 1.1585\n",
      "Epoch 28/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.4885 - val_loss: 1.1808\n",
      "Epoch 29/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.4846 - val_loss: 1.1676\n",
      "Epoch 30/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.4709 - val_loss: 1.1805\n",
      "Epoch 31/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.4568 - val_loss: 1.1552\n",
      "Epoch 32/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.4537 - val_loss: 1.1657\n",
      "Epoch 33/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.4433 - val_loss: 1.1764\n",
      "Epoch 34/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.4379 - val_loss: 1.1735\n",
      "Epoch 35/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.4256 - val_loss: 1.1760\n",
      "Epoch 36/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.4153 - val_loss: 1.1539\n",
      "Epoch 37/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.4156 - val_loss: 1.1550\n",
      "Epoch 38/64\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 0.3937 - val_loss: 1.1597\n",
      "Epoch 39/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.3960 - val_loss: 1.1850\n",
      "Epoch 40/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3898 - val_loss: 1.1744\n",
      "Epoch 41/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3887 - val_loss: 1.1739\n",
      "Epoch 42/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3775 - val_loss: 1.1624\n",
      "Epoch 43/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.3845 - val_loss: 1.1371\n",
      "Epoch 44/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.3868 - val_loss: 1.1618\n",
      "Epoch 45/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3784 - val_loss: 1.2014\n",
      "Epoch 46/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3726 - val_loss: 1.1688\n",
      "Epoch 47/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3813 - val_loss: 1.1591\n",
      "Epoch 48/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3813 - val_loss: 1.1234\n",
      "Epoch 49/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.3740 - val_loss: 1.1720\n",
      "Epoch 50/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.3660 - val_loss: 1.1924\n",
      "Epoch 51/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.3673 - val_loss: 1.2132\n",
      "Epoch 52/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.3706 - val_loss: 1.1178\n",
      "Epoch 53/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.3616 - val_loss: 1.1276\n",
      "Epoch 54/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.3511 - val_loss: 1.1477\n",
      "Epoch 55/64\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.3356 - val_loss: 1.1822\n",
      "Epoch 56/64\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 0.3516 - val_loss: 1.1365\n",
      "Epoch 57/64\n",
      "33/33 [==============================] - 0s 12ms/step - loss: 0.3414 - val_loss: 1.1297\n",
      "Epoch 58/64\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.3336 - val_loss: 1.1239\n",
      "Epoch 59/64\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.3335 - val_loss: 1.1662\n",
      "Epoch 60/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3282 - val_loss: 1.1713\n",
      "Epoch 61/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3469 - val_loss: 1.1010\n",
      "Epoch 62/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3284 - val_loss: 1.1110\n",
      "Epoch 63/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3225 - val_loss: 1.1578\n",
      "Epoch 64/64\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.3179 - val_loss: 1.1462\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Correlación de Pearson: 0.4246214799361913\n"
     ]
    }
   ],
   "source": [
    "model = create_and_train_and_evaluate_model(mapped, mapped_val,embedding_size=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uS_EyEA-8m74"
   },
   "source": [
    "## Train the same model with initiated trainable embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "841agYHG8m74"
   },
   "source": [
    "#### Random Embeddings (uniforme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8E5snLP8m74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kXfQwzz08m74"
   },
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiTeFTQe8m75"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9QkOR4Q28m75"
   },
   "source": [
    "## Analyze results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
