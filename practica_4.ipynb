{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C6R8Swq8m7f"
      },
      "source": [
        "# PrÃ ctica 4\n",
        "### Part I : Entrenament de models Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:00:29.701949Z",
          "start_time": "2023-05-31T17:00:23.336155Z"
        },
        "id": "1kdMxSds8m7h",
        "outputId": "7606a961-7a32-46bf-d6ef-d38f7f220e4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
            "[notice] To update, run: C:\\Users\\adria\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ca-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.5.0/ca_core_news_sm-3.5.0-py3-none-any.whl (19.6 MB)\n",
            "     --------------------------------------- 19.6/19.6 MB 38.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ca-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.31.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.10.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.23.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (65.5.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (4.6.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ca-core-news-sm==3.5.0) (2.1.1)\n",
            "Installing collected packages: ca-core-news-sm\n",
            "Successfully installed ca-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ca_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download ca_core_news_sm\n",
        "import spacy\n",
        "nlp = spacy.load('ca_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:08:44.005084Z",
          "start_time": "2023-05-31T17:00:30.215642Z"
        },
        "colab": {
          "referenced_widgets": [
            "3402ce7ea5f0473f8108e63ba971528e",
            "6318e79f477945a58838d50befe19df4",
            "915c1f7c98f74035983a90dc0e2c0ab7",
            "facac4f52c1045dda49157cbf06c76cf",
            "6031330d7a6c40aea7adbea8c2c2801c"
          ]
        },
        "id": "Q1Hp82XD8m7h",
        "outputId": "298e65da-3ee9-45a8-e8f0-2b53da3612f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3402ce7ea5f0473f8108e63ba971528e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6318e79f477945a58838d50befe19df4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.93k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset catalan_general_crawling/default to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "915c1f7c98f74035983a90dc0e2c0ab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "facac4f52c1045dda49157cbf06c76cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset catalan_general_crawling downloaded and prepared to C:/Users/adria/.cache/huggingface/datasets/projecte-aina___catalan_general_crawling/default/1.0.0/fb8a4dbf3849d7aad584ad030c058c67c6a935ee7b8d6e37411514da9376a2fb. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6031330d7a6c40aea7adbea8c2c2801c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ZN0P-78m7i"
      },
      "outputs": [],
      "source": [
        "with open('data/ca_gen_crwlng.txt','w',encoding='UTF-8') as f:\n",
        "    for i in dataset['train']['text']:\n",
        "        f.write(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JibG27ST8m7i"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from gensim.utils import simple_preprocess\n",
        "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
        "\n",
        "\n",
        "def preprocess(sentence: str) -> List[str]:\n",
        "    preprocessed = simple_preprocess(sentence)\n",
        "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAXGuZpa8m7i"
      },
      "outputs": [],
      "source": [
        "def obtain_words(filepath:str) -> list[list[str]]:\n",
        "    if filepath[-4:] != '.txt':\n",
        "        raise Exception('Incorrect file path/name')\n",
        "\n",
        "    txt_crps = []\n",
        "    with open(filepath,'r',encoding='UTF-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for l in lines:\n",
        "            txt_crps.append(preprocess(l))\n",
        "\n",
        "    return txt_crps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oqMe-N-8m7i"
      },
      "outputs": [],
      "source": [
        "txt_crps100 = obtain_words('data/ca_gen_crwlng100M.txt')\n",
        "txt_crps500 = obtain_words('data/ca_gen_crwlng500M.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8W_BLi98m7j"
      },
      "source": [
        "##### Create Word2Vec models with the 2 sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfD2XYGF8m7j"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "model100 = word2vec.Word2Vec(txt_crps100 , vector_size=100, window=5, min_count=10, workers=4, epochs=25, sg=1)\n",
        "model500 = word2vec.Word2Vec(txt_crps500 , vector_size=100, window=5, min_count=10, workers=4, epochs=25, sg=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ1y6zRK8m7j"
      },
      "source": [
        "##### Save the models to easy load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAGj-0Hq8m7j"
      },
      "outputs": [],
      "source": [
        "model100.wv.save_word2vec_format('data/word2vec100.bin', binary=True)\n",
        "model500.wv.save_word2vec_format('data/word2vec500.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_wGX5HP8m7j"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "wv100 = KeyedVectors.load_word2vec_format('data/word2vec100.bin', binary=True)\n",
        "wv500 = KeyedVectors.load_word2vec_format('data/word2vec500.bin', binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OXa5_sMn8m7k"
      },
      "source": [
        "## Text similarity training and comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhDnDtW18m7k"
      },
      "source": [
        "### Obtain the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d16735db18c34f5ba67e95abe726bf1c"
          ]
        },
        "id": "BgyI7KRR8m7k",
        "outputId": "46280675-b159-42c5-c063-3ec93caff855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset sts-ca (C:/Users/pelot/.cache/huggingface/datasets/projecte-aina___sts-ca/StsCa/1.0.2/bad37fb7fb0f06f3d2316e29637293b25160a93a24f36f1974f21313ac2f3342)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d16735db18c34f5ba67e95abe726bf1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"projecte-aina/sts-ca\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PBlhu8Eg8m7k"
      },
      "source": [
        "### Obtain the Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDhOJAu8m7k",
        "outputId": "9f516150-b25c-40db-a13e-5e08d4bfef77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 2073\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sOeh8Av8m7k"
      },
      "outputs": [],
      "source": [
        "## Imports necessaris\n",
        "##!python3 -m spacy download ca_core_news_md\n",
        "##!python3 -m spacy download ca_core_news_trf\n",
        "\n",
        "##!python3 -m pip install spacy-transformers\n",
        "##!python3 -m pip install sentence_transformers\n",
        "\n",
        "# Requisitos\n",
        "\n",
        "from gensim.models import TfidfModel, KeyedVectors, fasttext\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np\n",
        "# Tipado\n",
        "from typing import Tuple, List\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from scipy.stats import pearsonr\n",
        "from gensim.models.fasttext import FastTextKeyedVectors\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPqAOO7h8m7l",
        "outputId": "a7b96f2c-6c12-409f-908d-e75c5c31df32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfidf creadoo\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cargar stopwords en Catalan\n",
        "# STOPWORDS_CA = {\"a\", \"abans\", \"acÃ­\", \"ah\", \"aixÃ­\", \"aixÃ²\", \"al\", \"aleshores\", \"algun\", \"alguna\", \"algunes\", \"alguns\", \"alhora\", \"allÃ \", \"allÃ­\", \"allÃ²\", \"als\", \"altra\", \"altre\", \"altres\", \"amb\", \"ambdues\", \"ambdÃ³s\", \"anar\", \"ans\", \"apa\", \"aquell\", \"aquella\", \"aquelles\", \"aquells\", \"aquest\", \"aquesta\", \"aquestes\", \"aquests\", \"aquÃ­\", \"baix\", \"bastant\", \"bÃ©\", \"cada\", \"cadascuna\", \"cadascunes\", \"cadascuns\", \"cadascÃº\", \"com\", \"consegueixo\", \"conseguim\", \"conseguir\", \"consigueix\", \"consigueixen\", \"consigueixes\", \"contra\", \"d'un\", \"d'una\", \"d'unes\", \"d'uns\", \"dalt\", \"de\", \"del\", \"dels\", \"des\", \"des de\", \"desprÃ©s\", \"dins\", \"dintre\", \"donat\", \"doncs\", \"durant\", \"e\", \"eh\", \"el\", \"elles\", \"ells\", \"els\", \"em\", \"en\", \"encara\", \"ens\", \"entre\", \"era\", \"erem\", \"eren\", \"eres\", \"es\", \"esta\", \"estan\", \"estat\", \"estava\", \"estaven\", \"estem\", \"esteu\", \"estic\", \"estÃ \", \"estÃ vem\", \"estÃ veu\", \"et\", \"etc\", \"ets\", \"fa\", \"faig\", \"fan\", \"fas\", \"fem\", \"fer\", \"feu\", \"fi\", \"fins\", \"fora\", \"gairebÃ©\", \"ha\", \"han\", \"has\", \"haver\", \"havia\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \"i\", \"igual\", \"iguals\", \"inclÃ²s\", \"ja\", \"jo\", \"l'hi\", \"la\", \"les\", \"li\", \"li'n\", \"llarg\", \"llavors\", \"m'he\", \"ma\", \"mal\", \"malgrat\", \"mateix\", \"mateixa\", \"mateixes\", \"mateixos\", \"me\", \"mentre\", \"meu\", \"meus\", \"meva\", \"meves\", \"mode\", \"molt\", \"molta\", \"moltes\", \"molts\", \"mon\", \"mons\", \"mÃ©s\", \"n'he\", \"n'hi\", \"ne\", \"ni\", \"no\", \"nogensmenys\", \"nomÃ©s\", \"nosaltres\", \"nostra\", \"nostre\", \"nostres\", \"o\", \"oh\", \"oi\", \"on\", \"pas\", \"pel\", \"pels\", \"per\", \"per que\", \"perquÃ¨\", \"perÃ²\", \"poc\", \"poca\", \"pocs\", \"podem\", \"poden\", \"poder\", \"podeu\", \"poques\", \"potser\", \"primer\", \"propi\", \"puc\", \"qual\", \"quals\", \"quan\", \"quant\", \"que\", \"quelcom\", \"qui\", \"quin\", \"quina\", \"quines\", \"quins\", \"quÃ¨\", \"s'ha\", \"s'han\", \"sa\", \"sabem\", \"saben\", \"saber\", \"sabeu\", \"sap\", \"saps\", \"semblant\", \"semblants\", \"sense\", \"ser\", \"ses\", \"seu\", \"seus\", \"seva\", \"seves\", \"si\", \"sobre\", \"sobretot\", \"soc\", \"solament\", \"sols\", \"som\", \"son\", \"sons\", \"sota\", \"sou\", \"sÃ³c\", \"sÃ³n\", \"t'ha\", \"t'han\", \"t'he\", \"ta\", \"tal\", \"tambÃ©\", \"tampoc\", \"tan\", \"tant\", \"tanta\", \"tantes\", \"te\", \"tene\", \"tenim\", \"tenir\", \"teniu\", \"teu\", \"teus\", \"teva\", \"teves\", \"tinc\", \"ton\", \"tons\", \"tot\", \"tota\", \"totes\", \"tots\", \"un\", \"una\", \"unes\", \"uns\", \"us\", \"va\", \"vaig\", \"vam\", \"van\", \"vas\", \"veu\", \"vosaltres\", \"vostra\", \"vostre\", \"vostres\", \"Ã©rem\", \"Ã©reu\", \"Ã©s\", \"Ã©ssent\", \"Ãºltim\", \"Ãºs\"}\n",
        "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
        "\n",
        "# Definir funciÃ³n de pre-procesado\n",
        "def preprocess(sentence: str) -> List[str]:\n",
        "    preprocessed = simple_preprocess(sentence)\n",
        "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
        "    return preprocessed\n",
        "\n",
        "## Introducir los datos de train y de validaciÃ³n\n",
        "input_pairs = list(zip(dataset[\"train\"][\"sentence1\"], dataset[\"train\"][\"sentence2\"], dataset[\"train\"][\"label\"]))\n",
        "input_pairs_val = list(zip(dataset[\"validation\"][\"sentence1\"], dataset[\"validation\"][\"sentence2\"], dataset[\"validation\"][\"label\"]))\n",
        "\n",
        "\n",
        "# Preprocesamiento de las oraciones y creaciÃ³n del diccionario\n",
        "sentences_1_preproc = [preprocess(sentence_1) for sentence_1, _, _ in input_pairs]\n",
        "sentences_2_preproc = [preprocess(sentence_2) for _, sentence_2, _ in input_pairs]\n",
        "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
        "# VersiÃ³n aplanada para poder entrenar el modelo\n",
        "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
        "\n",
        "diccionario = Dictionary(sentences_pairs_flattened)\n",
        "\n",
        "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
        "modelo_tfidf = TfidfModel(corpus)\n",
        "\n",
        "def creacio_one_hot(sentence, diccionario:Dictionary):\n",
        "    vector1 = np.zeros(len(diccionario.token2id), dtype=np.float)\n",
        "    bow1 = diccionario.doc2bow(sentence)\n",
        "    for index, count in bow1:\n",
        "        vector1[index] = count\n",
        "\n",
        "    return vector1\n",
        "\n",
        "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
        "    bow = dictionary.doc2bow(sentence_preproc)\n",
        "    tf_idf = tf_idf_model[bow]\n",
        "    vectors, weights = [], []\n",
        "    for word_index, weight in tf_idf:\n",
        "        word = dictionary.get(word_index)\n",
        "        if word in wv_model:\n",
        "            vectors.append(wv_model[word])\n",
        "            weights.append(weight)\n",
        "    return vectors, weights\n",
        "\n",
        "def map_pairs(\n",
        "        sentence_pairs: List[Tuple[str, str, float]],\n",
        "        dictionary: Dictionary = None,\n",
        "        tf_idf_model: TfidfModel = None,\n",
        "        one_hot: bool = None,\n",
        "        Spasii : bool = None,\n",
        "        RobertA: bool = None,\n",
        "        RobertA_Mean: bool = None,\n",
        "        wv100: bool = None,\n",
        "        wv500: bool = None,\n",
        "        w2v_pre_mean: bool = None,\n",
        "        RobertA_finetuned: bool = None\n",
        ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
        "    \"\"\"\n",
        "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
        "    :param sentence_pairs:\n",
        "    :param dictionary:\n",
        "    :param tf_idf_model:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if Spasii:\n",
        "        nlp = spacy.load(\"ca_core_news_md\")\n",
        "    if RobertA:\n",
        "        nlp = spacy.load(\"ca_core_news_trf\")\n",
        "\n",
        "    if wv100:\n",
        "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec100.bin\", binary = True)\n",
        "\n",
        "    if wv500:\n",
        "        wv_model = KeyedVectors.load_word2vec_format(\"data/word2vec500.bin\", binary = True)\n",
        "\n",
        "    if w2v_pre_mean or tf_idf_model:\n",
        "        wv_model = FastTextKeyedVectors.load('data/model.bin', mmap='r')\n",
        "\n",
        "    if RobertA_finetuned:\n",
        "        model_name = 'stsb-roberta-base'\n",
        "        model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Mapeo de los pares de oraciones a pares de vectores\n",
        "    pares_vectores = []\n",
        "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
        "        sentence_1_preproc = preprocess(sentence_1)\n",
        "        sentence_2_preproc = preprocess(sentence_2)\n",
        "        # Si usamos TF-IDF\n",
        "        if tf_idf_model is not None:\n",
        "            # CÃ¡lculo del promedio ponderado por TF-IDF de los word embeddings\n",
        "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, wv_model=wv_model )\n",
        "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model,wv_model=wv_model )\n",
        "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
        "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
        "\n",
        "        elif wv100 is not None:\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "\n",
        "        elif wv500 is not None:\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "\n",
        "        elif one_hot is not None:\n",
        "            vector1 = creacio_one_hot(sentence=sentence_1_preproc, diccionario=dictionary)\n",
        "            vector2 = creacio_one_hot(sentence=sentence_2_preproc, diccionario= dictionary)\n",
        "            \n",
        "        elif Spasii is not None:\n",
        "            vector1 = nlp(\" \".join(sentence_1_preproc)).vector\n",
        "            vector2 = nlp(\" \".join(sentence_2_preproc)).vector\n",
        "\n",
        "        elif RobertA is not None:\n",
        "            vector1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
        "            vector2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
        "\n",
        "        elif RobertA_Mean is not None:\n",
        "            vectors1 = nlp(\" \".join(sentence_1_preproc))._.trf_data.tensors[-1]\n",
        "            vectors2 = nlp(\" \".join(sentence_2_preproc))._.trf_data.tensors[-1]\n",
        "\n",
        "            vector1 = np.average(vectors1, axis=0)\n",
        "            vector2 = np.average(vectors2, axis=0)\n",
        "\n",
        "        elif RobertA_finetuned is not None:\n",
        "            vector1 = model.encode([\" \".join(sentence_1_preproc)])\n",
        "            vector2 = model.encode([\" \".join(sentence_2_preproc)])\n",
        "            \n",
        "        elif w2v_pre_mean is not None:\n",
        "            # CÃ¡lculo del promedio de los word embeddings\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "        # AÃ±adir a la lista\n",
        "        pares_vectores.append(((vector1, vector2), similitud))\n",
        "    return pares_vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqBiQ_7R8m7l"
      },
      "source": [
        "Fem la funciÃ³ de la creaciÃ³ del model y dels hiperparÃ¡metres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyNL-8mQ8m7m"
      },
      "outputs": [],
      "source": [
        "def build_and_compile_model(hidden_size: int = 128, embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
        "    # Capa de entrada para los pares de vectores\n",
        "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
        "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
        "\n",
        "    # Capa oculta\n",
        "    first_projection = tf.keras.layers.Dense(\n",
        "        embedding_size,\n",
        "        # activation='tanh',\n",
        "        kernel_initializer=tf.keras.initializers.Identity(),\n",
        "        bias_initializer=tf.keras.initializers.Zeros(),\n",
        "    )\n",
        "    projected_1 = first_projection(input_1)\n",
        "    projected_2 = first_projection(input_2)\n",
        "\n",
        "    # Compute the cosine distance\n",
        "    projected_1 = tf.linalg.l2_normalize(projected_1, axis=1, )\n",
        "    projected_2 = tf.linalg.l2_normalize(projected_2, axis=1, )\n",
        "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
        "\n",
        "    # Definir el modelo con las capas de entrada y salida\n",
        "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(loss='mean_absolute_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definir constantes de entrenamiento\n",
        "batch_size: int = 64\n",
        "num_epochs: int = 64\n",
        "\n",
        "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
        "    :param pair_list:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    _x, _y = zip(*pair_list)\n",
        "    _x_1, _x_2 = zip(*_x)\n",
        "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )\n",
        "\n",
        "\n",
        "def create_and_train_and_evaluate_model(mapped, mapped_val):\n",
        "    # Obtener las listas de train y test\n",
        "    x_train, y_train = pair_list_to_x_y(mapped)\n",
        "    x_val, y_val = pair_list_to_x_y(mapped_val)\n",
        "\n",
        "    # Preparar los conjuntos de datos de entrenamiento y validaciÃ³n\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Construir y compilar el modelo\n",
        "    model = build_and_compile_model()\n",
        "    # tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, )\n",
        "    print(model.summary())\n",
        "    # Entrenar el modelo\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "    y_pred: tf.RaggedTensor = model.predict(x_val)\n",
        "    # Calcular la correlaciÃ³n de Pearson entre las predicciones y los datos de prueba\n",
        "    correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
        "    # Imprimir el coeficiente de correlaciÃ³n de Pearson\n",
        "    print(f\"CorrelaciÃ³n de Pearson: {correlation}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zvHCfM7_8m7m"
      },
      "source": [
        "### Compare results with different word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZtqXFR8m7m"
      },
      "source": [
        "#### 0. Model de Word2Vec entrenats per nosaltres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A3IweZK8m7m"
      },
      "source": [
        "##### 0.1. Model amb 100 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZENU8kuc8m7m"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, wv100=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, wv100=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n7zMRP38m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-pSYQe8m7n"
      },
      "source": [
        "##### 0.2. Model amb 500 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxx6u1Ig8m7n"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, wv500=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, wv500=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmOYMJ048m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QF0EG-AD8m7n"
      },
      "source": [
        "#### 1. One Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z55O8jG8m7n"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, one_hot=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, one_hot=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoNQhiIg8m7n"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dmfqVPvY8m7n"
      },
      "source": [
        "#### 2. Models de Word2Vec/GloVe pre-entrenats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FGELWVb88m7n"
      },
      "source": [
        "##### 2.1.Word2Vec + Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvjn7pJL8m7o"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, w2v_pre_mean=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, w2v_pre_mean=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHRxNJ_J8m7o"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oDA0wr-X8m7o"
      },
      "source": [
        "##### 2.2.Word2Vec + Mean ponderada (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8zH2ip08m7o"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSMyClBN8m7o"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GsHl1GQD8m72"
      },
      "source": [
        "#### 3.Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd1Oecv8m72"
      },
      "source": [
        "Primer definim els embeddings amb el model de Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Q4cmr38m72"
      },
      "outputs": [],
      "source": [
        "mapped = map_pairs(input_pairs, Spasii=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, Spasii=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juK2JoYz8m72"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SkeQ57oO8m72"
      },
      "source": [
        "#### 4.RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CBGLTSu28m73"
      },
      "source": [
        "##### 4.1.CLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjFoNCWB8m73"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClT4pyhg8m73"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d5qL6zGN8m73"
      },
      "source": [
        "##### 4.2.MEAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCxEI7aH8m73"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA_Mean=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA_Mean=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaDmE7D88m73"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9GYDsChM8m73"
      },
      "source": [
        "#### 5.RoBERTa FineTuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T17:48:42.264845Z",
          "start_time": "2023-05-31T17:11:49.470503Z"
        },
        "id": "OZd_7rU-8m74"
      },
      "outputs": [],
      "source": [
        "# (Amb spaCy, doc._.trf_data.tensors[-1])\n",
        "mapped = map_pairs(input_pairs, RobertA_finetuned=True, dictionary=diccionario )\n",
        "mapped_val = map_pairs(input_pairs_val, RobertA_finetuned=True, dictionary=diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnrRULCk8m74"
      },
      "outputs": [],
      "source": [
        "model = create_and_train_and_evaluate_model(mapped, mapped_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uS_EyEA-8m74"
      },
      "source": [
        "## Train the same model with initiated trainable embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "841agYHG8m74"
      },
      "source": [
        "#### Random Embeddings (uniforme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8E5snLP8m74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kXfQwzz08m74"
      },
      "source": [
        "#### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiTeFTQe8m75"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9QkOR4Q28m75"
      },
      "source": [
        "## Analyze results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
